{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/Train_call.txt',delimiter = '\\t')\n",
    "labels = pd.read_csv('../data/Train_clinical.txt',delimiter = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chromosome</th>\n",
       "      <th>Start</th>\n",
       "      <th>End</th>\n",
       "      <th>Nclone</th>\n",
       "      <th>Array.129</th>\n",
       "      <th>Array.34</th>\n",
       "      <th>Array.67</th>\n",
       "      <th>Array.24</th>\n",
       "      <th>Array.22</th>\n",
       "      <th>Array.36</th>\n",
       "      <th>...</th>\n",
       "      <th>Array.64</th>\n",
       "      <th>Array.89</th>\n",
       "      <th>Array.30</th>\n",
       "      <th>Array.35</th>\n",
       "      <th>Array.93</th>\n",
       "      <th>Array.10</th>\n",
       "      <th>Array.123</th>\n",
       "      <th>Array.100</th>\n",
       "      <th>Array.134</th>\n",
       "      <th>Array.130</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2927</td>\n",
       "      <td>43870</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85022</td>\n",
       "      <td>216735</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>370546</td>\n",
       "      <td>372295</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>471671</td>\n",
       "      <td>786483</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>792533</td>\n",
       "      <td>907406</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Chromosome   Start     End  Nclone  Array.129  Array.34  Array.67  \\\n",
       "0           1    2927   43870       3          0         0         0   \n",
       "1           1   85022  216735       4          0         0         0   \n",
       "2           1  370546  372295       4          0         0         0   \n",
       "3           1  471671  786483       5          0         0         0   \n",
       "4           1  792533  907406      13          0         0         0   \n",
       "\n",
       "   Array.24  Array.22  Array.36  ...  Array.64  Array.89  Array.30  Array.35  \\\n",
       "0         0         0         0  ...         0         0         1         0   \n",
       "1         0         0         0  ...         0         0         1         0   \n",
       "2         0         0         0  ...         0         0         1         0   \n",
       "3         0         0         0  ...         0         1         1         0   \n",
       "4         0         0         0  ...         0         1         1         0   \n",
       "\n",
       "   Array.93  Array.10  Array.123  Array.100  Array.134  Array.130  \n",
       "0         0         0          0          0         -1          0  \n",
       "1         0         0          0          0         -1          0  \n",
       "2         1         0          0          0         -1          0  \n",
       "3         1         0          0          0         -1          0  \n",
       "4         1         0          0          0         -1          0  \n",
       "\n",
       "[5 rows x 104 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2824</th>\n",
       "      <th>2825</th>\n",
       "      <th>2826</th>\n",
       "      <th>2827</th>\n",
       "      <th>2828</th>\n",
       "      <th>2829</th>\n",
       "      <th>2830</th>\n",
       "      <th>2831</th>\n",
       "      <th>2832</th>\n",
       "      <th>2833</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Chromosome</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Start</th>\n",
       "      <td>2927</td>\n",
       "      <td>85022</td>\n",
       "      <td>370546</td>\n",
       "      <td>471671</td>\n",
       "      <td>792533</td>\n",
       "      <td>912799</td>\n",
       "      <td>1271190</td>\n",
       "      <td>1676445</td>\n",
       "      <td>1738295</td>\n",
       "      <td>2481927</td>\n",
       "      <td>...</td>\n",
       "      <td>151067607</td>\n",
       "      <td>152422390</td>\n",
       "      <td>152552851</td>\n",
       "      <td>152576854</td>\n",
       "      <td>152994680</td>\n",
       "      <td>153062077</td>\n",
       "      <td>153466463</td>\n",
       "      <td>153504394</td>\n",
       "      <td>153938998</td>\n",
       "      <td>153997146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>End</th>\n",
       "      <td>43870</td>\n",
       "      <td>216735</td>\n",
       "      <td>372295</td>\n",
       "      <td>786483</td>\n",
       "      <td>907406</td>\n",
       "      <td>1266212</td>\n",
       "      <td>1590570</td>\n",
       "      <td>1703748</td>\n",
       "      <td>2477597</td>\n",
       "      <td>2562342</td>\n",
       "      <td>...</td>\n",
       "      <td>152416606</td>\n",
       "      <td>152548587</td>\n",
       "      <td>152570071</td>\n",
       "      <td>152935130</td>\n",
       "      <td>153054487</td>\n",
       "      <td>153452633</td>\n",
       "      <td>153491568</td>\n",
       "      <td>153933426</td>\n",
       "      <td>153989329</td>\n",
       "      <td>154492924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nclone</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>96</td>\n",
       "      <td>45</td>\n",
       "      <td>4</td>\n",
       "      <td>104</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>159</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>57</td>\n",
       "      <td>4</td>\n",
       "      <td>55</td>\n",
       "      <td>5</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Array.129</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Array.10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Array.123</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Array.100</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Array.134</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Array.130</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>104 rows Ã— 2834 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0       1       2       3       4        5        6        7     \\\n",
       "Chromosome      1       1       1       1       1        1        1        1   \n",
       "Start        2927   85022  370546  471671  792533   912799  1271190  1676445   \n",
       "End         43870  216735  372295  786483  907406  1266212  1590570  1703748   \n",
       "Nclone          3       4       4       5      13       96       45        4   \n",
       "Array.129       0       0       0       0       0        0        0        0   \n",
       "...           ...     ...     ...     ...     ...      ...      ...      ...   \n",
       "Array.10        0       0       0       0       0        0        0        0   \n",
       "Array.123       0       0       0       0       0        0        0        0   \n",
       "Array.100       0       0       0       0       0        0        0        0   \n",
       "Array.134      -1      -1      -1      -1      -1       -1       -1       -1   \n",
       "Array.130       0       0       0       0       0        0        0        0   \n",
       "\n",
       "               8        9     ...       2824       2825       2826       2827  \\\n",
       "Chromosome        1        1  ...         23         23         23         23   \n",
       "Start       1738295  2481927  ...  151067607  152422390  152552851  152576854   \n",
       "End         2477597  2562342  ...  152416606  152548587  152570071  152935130   \n",
       "Nclone          104       14  ...        159         20          6         64   \n",
       "Array.129         0        0  ...          2          2          2          2   \n",
       "...             ...      ...  ...        ...        ...        ...        ...   \n",
       "Array.10          0        0  ...          1          1          0          1   \n",
       "Array.123         0        0  ...          1          1          1          1   \n",
       "Array.100         0        0  ...          1          1          1          1   \n",
       "Array.134        -1       -1  ...          1          1          1          1   \n",
       "Array.130         0        0  ...          1          1          1          1   \n",
       "\n",
       "                 2828       2829       2830       2831       2832       2833  \n",
       "Chromosome         23         23         23         23         23         23  \n",
       "Start       152994680  153062077  153466463  153504394  153938998  153997146  \n",
       "End         153054487  153452633  153491568  153933426  153989329  154492924  \n",
       "Nclone              5         57          4         55          5         43  \n",
       "Array.129           0          1          1          1          1          1  \n",
       "...               ...        ...        ...        ...        ...        ...  \n",
       "Array.10            1          1          1          1          1          1  \n",
       "Array.123           1          1          1          1          1          1  \n",
       "Array.100           1          1          1          1          1          1  \n",
       "Array.134           1          1          1          1          1          1  \n",
       "Array.130           1          1          1          1          1          1  \n",
       "\n",
       "[104 rows x 2834 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.transpose()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2824</th>\n",
       "      <th>2825</th>\n",
       "      <th>2826</th>\n",
       "      <th>2827</th>\n",
       "      <th>2828</th>\n",
       "      <th>2829</th>\n",
       "      <th>2830</th>\n",
       "      <th>2831</th>\n",
       "      <th>2832</th>\n",
       "      <th>2833</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Array.129</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Array.34</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Array.67</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Array.24</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Array.22</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Array.10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Array.123</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Array.100</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Array.134</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Array.130</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 2834 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0     1     2     3     4     5     6     7     8     9     ...  \\\n",
       "Array.129     0     0     0     0     0     0     0     0     0     0  ...   \n",
       "Array.34      0     0     0     0     0     0     0     0     0     0  ...   \n",
       "Array.67      0     0     0     0     0     0     0     0     0     0  ...   \n",
       "Array.24      0     0     0     0     0     0     0    -1     0     0  ...   \n",
       "Array.22      0     0     0     0     0     0     0     0     0     0  ...   \n",
       "...         ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   \n",
       "Array.10      0     0     0     0     0     0     0     0     0     0  ...   \n",
       "Array.123     0     0     0     0     0     0     0     0     0     0  ...   \n",
       "Array.100     0     0     0     0     0     0     0     0     0     0  ...   \n",
       "Array.134    -1    -1    -1    -1    -1    -1    -1    -1    -1    -1  ...   \n",
       "Array.130     0     0     0     0     0     0     0     0     0     0  ...   \n",
       "\n",
       "           2824  2825  2826  2827  2828  2829  2830  2831  2832  2833  \n",
       "Array.129     2     2     2     2     0     1     1     1     1     1  \n",
       "Array.34      1     1     1     1     1     1     1     1     1     1  \n",
       "Array.67      1     1     1     1     1     1     1     1     1     1  \n",
       "Array.24      0     0     0     0     0     0     0     0     0     0  \n",
       "Array.22      1     1     1     1     1     1     1     1     1     1  \n",
       "...         ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "Array.10      1     1     0     1     1     1     1     1     1     1  \n",
       "Array.123     1     1     1     1     1     1     1     1     1     1  \n",
       "Array.100     1     1     1     1     1     1     1     1     1     1  \n",
       "Array.134     1     1     1     1     1     1     1     1     1     1  \n",
       "Array.130     1     1     1     1     1     1     1     1     1     1  \n",
       "\n",
       "[100 rows x 2834 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[4:]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>2824</th>\n",
       "      <th>2825</th>\n",
       "      <th>2826</th>\n",
       "      <th>2827</th>\n",
       "      <th>2828</th>\n",
       "      <th>2829</th>\n",
       "      <th>2830</th>\n",
       "      <th>2831</th>\n",
       "      <th>2832</th>\n",
       "      <th>2833</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Array.129</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Array.34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Array.67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Array.24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Array.22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Array.10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Array.123</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Array.100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Array.134</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Array.130</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 2835 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index  0  1  2  3  4  5  6  7  8  ...  2824  2825  2826  2827  2828  \\\n",
       "0   Array.129  0  0  0  0  0  0  0  0  0  ...     2     2     2     2     0   \n",
       "1    Array.34  0  0  0  0  0  0  0  0  0  ...     1     1     1     1     1   \n",
       "2    Array.67  0  0  0  0  0  0  0  0  0  ...     1     1     1     1     1   \n",
       "3    Array.24  0  0  0  0  0  0  0 -1  0  ...     0     0     0     0     0   \n",
       "4    Array.22  0  0  0  0  0  0  0  0  0  ...     1     1     1     1     1   \n",
       "..        ... .. .. .. .. .. .. .. .. ..  ...   ...   ...   ...   ...   ...   \n",
       "95   Array.10  0  0  0  0  0  0  0  0  0  ...     1     1     0     1     1   \n",
       "96  Array.123  0  0  0  0  0  0  0  0  0  ...     1     1     1     1     1   \n",
       "97  Array.100  0  0  0  0  0  0  0  0  0  ...     1     1     1     1     1   \n",
       "98  Array.134 -1 -1 -1 -1 -1 -1 -1 -1 -1  ...     1     1     1     1     1   \n",
       "99  Array.130  0  0  0  0  0  0  0  0  0  ...     1     1     1     1     1   \n",
       "\n",
       "    2829  2830  2831  2832  2833  \n",
       "0      1     1     1     1     1  \n",
       "1      1     1     1     1     1  \n",
       "2      1     1     1     1     1  \n",
       "3      0     0     0     0     0  \n",
       "4      1     1     1     1     1  \n",
       "..   ...   ...   ...   ...   ...  \n",
       "95     1     1     1     1     1  \n",
       "96     1     1     1     1     1  \n",
       "97     1     1     1     1     1  \n",
       "98     1     1     1     1     1  \n",
       "99     1     1     1     1     1  \n",
       "\n",
       "[100 rows x 2835 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.reset_index()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>2824</th>\n",
       "      <th>2825</th>\n",
       "      <th>2826</th>\n",
       "      <th>2827</th>\n",
       "      <th>2828</th>\n",
       "      <th>2829</th>\n",
       "      <th>2830</th>\n",
       "      <th>2831</th>\n",
       "      <th>2832</th>\n",
       "      <th>2833</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Array.129</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Array.34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Array.67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Array.24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Array.22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2835 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sample  0  1  2  3  4  5  6  7  8  ...  2824  2825  2826  2827  2828  \\\n",
       "0  Array.129  0  0  0  0  0  0  0  0  0  ...     2     2     2     2     0   \n",
       "1   Array.34  0  0  0  0  0  0  0  0  0  ...     1     1     1     1     1   \n",
       "2   Array.67  0  0  0  0  0  0  0  0  0  ...     1     1     1     1     1   \n",
       "3   Array.24  0  0  0  0  0  0  0 -1  0  ...     0     0     0     0     0   \n",
       "4   Array.22  0  0  0  0  0  0  0  0  0  ...     1     1     1     1     1   \n",
       "\n",
       "   2829  2830  2831  2832  2833  \n",
       "0     1     1     1     1     1  \n",
       "1     1     1     1     1     1  \n",
       "2     1     1     1     1     1  \n",
       "3     0     0     0     0     0  \n",
       "4     1     1     1     1     1  \n",
       "\n",
       "[5 rows x 2835 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.rename(columns={'index':'Sample'}, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample</th>\n",
       "      <th>Subgroup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Array.129</td>\n",
       "      <td>HER2+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Array.34</td>\n",
       "      <td>HR+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Array.67</td>\n",
       "      <td>HR+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Array.24</td>\n",
       "      <td>Triple Neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Array.22</td>\n",
       "      <td>Triple Neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sample    Subgroup\n",
       "0  Array.129       HER2+\n",
       "1   Array.34         HR+\n",
       "2   Array.67         HR+\n",
       "3   Array.24  Triple Neg\n",
       "4   Array.22  Triple Neg"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>2825</th>\n",
       "      <th>2826</th>\n",
       "      <th>2827</th>\n",
       "      <th>2828</th>\n",
       "      <th>2829</th>\n",
       "      <th>2830</th>\n",
       "      <th>2831</th>\n",
       "      <th>2832</th>\n",
       "      <th>2833</th>\n",
       "      <th>Subgroup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Array.129</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>HER2+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Array.34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>HR+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Array.67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>HR+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Array.24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Triple Neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Array.22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Triple Neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Array.10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>HER2+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Array.123</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>HR+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Array.100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>HR+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Array.134</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>HR+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Array.130</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>HER2+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 2836 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sample  0  1  2  3  4  5  6  7  8  ...  2825  2826  2827  2828  2829  \\\n",
       "0   Array.129  0  0  0  0  0  0  0  0  0  ...     2     2     2     0     1   \n",
       "1    Array.34  0  0  0  0  0  0  0  0  0  ...     1     1     1     1     1   \n",
       "2    Array.67  0  0  0  0  0  0  0  0  0  ...     1     1     1     1     1   \n",
       "3    Array.24  0  0  0  0  0  0  0 -1  0  ...     0     0     0     0     0   \n",
       "4    Array.22  0  0  0  0  0  0  0  0  0  ...     1     1     1     1     1   \n",
       "..        ... .. .. .. .. .. .. .. .. ..  ...   ...   ...   ...   ...   ...   \n",
       "95   Array.10  0  0  0  0  0  0  0  0  0  ...     1     0     1     1     1   \n",
       "96  Array.123  0  0  0  0  0  0  0  0  0  ...     1     1     1     1     1   \n",
       "97  Array.100  0  0  0  0  0  0  0  0  0  ...     1     1     1     1     1   \n",
       "98  Array.134 -1 -1 -1 -1 -1 -1 -1 -1 -1  ...     1     1     1     1     1   \n",
       "99  Array.130  0  0  0  0  0  0  0  0  0  ...     1     1     1     1     1   \n",
       "\n",
       "    2830  2831  2832  2833    Subgroup  \n",
       "0      1     1     1     1       HER2+  \n",
       "1      1     1     1     1         HR+  \n",
       "2      1     1     1     1         HR+  \n",
       "3      0     0     0     0  Triple Neg  \n",
       "4      1     1     1     1  Triple Neg  \n",
       "..   ...   ...   ...   ...         ...  \n",
       "95     1     1     1     1       HER2+  \n",
       "96     1     1     1     1         HR+  \n",
       "97     1     1     1     1         HR+  \n",
       "98     1     1     1     1         HR+  \n",
       "99     1     1     1     1       HER2+  \n",
       "\n",
       "[100 rows x 2836 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine labels and instances\n",
    "combined = pd.merge(data,labels,on='Sample',)\n",
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>2825</th>\n",
       "      <th>2826</th>\n",
       "      <th>2827</th>\n",
       "      <th>2828</th>\n",
       "      <th>2829</th>\n",
       "      <th>2830</th>\n",
       "      <th>2831</th>\n",
       "      <th>2832</th>\n",
       "      <th>2833</th>\n",
       "      <th>Subgroup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Array.129</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>HER2+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Array.34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>HR+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Array.67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>HR+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Array.24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Triple Neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Array.22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Triple Neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2836 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sample  0  1  2  3  4  5  6  7  8  ...  2825  2826  2827  2828  2829  \\\n",
       "0  Array.129  0  0  0  0  0  0  0  0  0  ...     2     2     2     0     1   \n",
       "1   Array.34  0  0  0  0  0  0  0  0  0  ...     1     1     1     1     1   \n",
       "2   Array.67  0  0  0  0  0  0  0  0  0  ...     1     1     1     1     1   \n",
       "3   Array.24  0  0  0  0  0  0  0 -1  0  ...     0     0     0     0     0   \n",
       "4   Array.22  0  0  0  0  0  0  0  0  0  ...     1     1     1     1     1   \n",
       "\n",
       "   2830  2831  2832  2833    Subgroup  \n",
       "0     1     1     1     1       HER2+  \n",
       "1     1     1     1     1         HR+  \n",
       "2     1     1     1     1         HR+  \n",
       "3     0     0     0     0  Triple Neg  \n",
       "4     1     1     1     1  Triple Neg  \n",
       "\n",
       "[5 rows x 2836 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = combined.iloc[:,1:2835]\n",
    "y = combined['Subgroup']\n",
    "\n",
    "combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HR+           36\n",
       "HER2+         32\n",
       "Triple Neg    32\n",
       "Name: Subgroup, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if class lables are balanced or not\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (80, 2834)\n",
      "X_test (20, 2834)\n",
      "y_train (80,)\n",
      "y_test (20,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.20, random_state=42)\n",
    "print('X_train', X_train.shape)\n",
    "print('X_test', X_test.shape)\n",
    "print('y_train', y_train.shape)\n",
    "print('y_test', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HR+           29\n",
       "Triple Neg    26\n",
       "HER2+         25\n",
       "Name: Subgroup, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HR+           7\n",
       "HER2+         7\n",
       "Triple Neg    6\n",
       "Name: Subgroup, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multinomial logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the RFE object and compute a cross-validated score.\n",
    "# lr = LogisticRegression(multi_class='multinomial')\n",
    "# # The \"accuracy\" scoring shows the proportion of correct classifications\n",
    "\n",
    "# min_features_to_select = 1  # Minimum number of features to consider\n",
    "# rfecv_lr = RFECV(\n",
    "#     estimator=lr,\n",
    "#     step=1,\n",
    "#     cv=StratifiedKFold(5),\n",
    "#     scoring=\"accuracy\",\n",
    "#     min_features_to_select=min_features_to_select,\n",
    "# )\n",
    "# rfecv_lr.fit(X_train, y_train)\n",
    "\n",
    "# print(\"Optimal number of features : %d\" % rfecv_lr.n_features_)\n",
    "\n",
    "# # Plot number of features VS. cross-validation scores\n",
    "# plt.figure()\n",
    "# plt.xlabel(\"Number of features selected\")\n",
    "# plt.ylabel(\"Cross validation score (accuracy)\")\n",
    "# plt.plot(\n",
    "#     range(min_features_to_select, len(rfecv_lr.grid_scores_) + min_features_to_select),\n",
    "#     rfecv_lr.grid_scores_,\n",
    "# )\n",
    "# plt.title(\"RFECV of Multinomial Logistic Regression\")\n",
    "# plt.legend(['1cv', '2cv', '3cv', '4cv', '5cv'], loc=4)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # selected features\n",
    "# fs_lr = rfecv_lr.get_feature_names_out()\n",
    "# fs_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get selected features for training set\n",
    "# X_train_fs_lr = pd.DataFrame(rfecv_lr.transform(X_train))\n",
    "\n",
    "# # rename new dataframe for training set\n",
    "# for i, col in enumerate(X_train_fs_lr.columns):\n",
    "#     X_train_fs_lr.rename(columns={col:fs_lr[i]}, inplace=True)\n",
    "\n",
    "# X_train_fs_lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get selected features for test set\n",
    "# X_test_fs_lr = pd.DataFrame(rfecv_lr.transform(X_test))\n",
    "\n",
    "# # rename new dataframe for test set\n",
    "# for i, col in enumerate(X_test_fs_lr.columns):\n",
    "#     X_test_fs_lr.rename(columns={col:fs_lr[i]}, inplace=True)\n",
    "\n",
    "# X_test_fs_lr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the RFE object and compute a cross-validated score.\n",
    "# rf = RandomForestClassifier(random_state = 42)\n",
    "# # The \"accuracy\" scoring shows the proportion of correct classifications\n",
    "\n",
    "# min_features_to_select = 1  # Minimum number of features to consider\n",
    "# rfecv_rf = RFECV(\n",
    "#     estimator=rf,\n",
    "#     step=1,\n",
    "#     cv=StratifiedKFold(5),\n",
    "#     scoring=\"accuracy\",\n",
    "#     min_features_to_select=min_features_to_select,\n",
    "# )\n",
    "# rfecv_rf.fit(X_train, y_train)\n",
    "\n",
    "# print(\"Optimal number of features : %d\" % rfecv_rf.n_features_)\n",
    "\n",
    "# # Plot number of features VS. cross-validation scores\n",
    "# plt.xlabel(\"Number of features selected\")\n",
    "# plt.ylabel(\"Cross validation score (accuracy)\")\n",
    "# plt.plot(\n",
    "#     range(min_features_to_select, len(rfecv_rf.grid_scores_) + min_features_to_select),\n",
    "#     rfecv_rf.grid_scores_,\n",
    "# )\n",
    "# plt.title(\"RFECV of Random Forest\")\n",
    "# plt.legend(['1cv', '2cv', '3cv', '4cv', '5cv'], loc=4)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55    Triple Neg\n",
       "88    Triple Neg\n",
       "26         HER2+\n",
       "42           HR+\n",
       "69           HR+\n",
       "         ...    \n",
       "60         HER2+\n",
       "71         HER2+\n",
       "14    Triple Neg\n",
       "92    Triple Neg\n",
       "51           HR+\n",
       "Name: Subgroup, Length: 80, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encode y_train for xgboost\n",
    "lc = LabelEncoder() \n",
    "lc = lc.fit(y_train) \n",
    "lc_y_train = lc.transform(y_train)\n",
    "\n",
    "# transform lc_y_train into series\n",
    "lc_y_train = pd.Series(lc_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encode y_test\n",
    "lc1 = lc.fit(y_test) \n",
    "lc_y_test = lc1.transform(y_test)\n",
    "\n",
    "# transform lc_y_test into series\n",
    "lc_y_test = pd.Series(lc_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the RFE object and compute a cross-validated score.\n",
    "# xgb = XGBClassifier(random_state=42)\n",
    "# # The \"accuracy\" scoring shows the proportion of correct classifications\n",
    "\n",
    "# min_features_to_select = 1  # Minimum number of features to consider\n",
    "# rfecv_xgb = RFECV(\n",
    "#     estimator=xgb,\n",
    "#     step=1,\n",
    "#     cv=StratifiedKFold(5),\n",
    "#     scoring=\"accuracy\",\n",
    "#     min_features_to_select=min_features_to_select,\n",
    "# )\n",
    "# rfecv_xgb.fit(X_train, lc_y_train)\n",
    "\n",
    "# print(\"Optimal number of features : %d\" % rfecv_xgb.n_features_)\n",
    "\n",
    "# # Plot number of features VS. cross-validation scores\n",
    "# plt.figure()\n",
    "# plt.xlabel(\"Number of features selected\")\n",
    "# plt.ylabel(\"Cross validation score (accuracy)\")\n",
    "# plt.plot(\n",
    "#     range(min_features_to_select, len(rfecv_xgb.grid_scores_) + min_features_to_select),\n",
    "#     rfecv_xgb.grid_scores_,\n",
    "# )\n",
    "# plt.title(\"RFECV of XGBoost\")\n",
    "# plt.legend(['1cv', '2cv', '3cv', '4cv', '5cv'], loc=4)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # selected features\n",
    "# fs_xgb = rfecv_xgb.get_feature_names_out()\n",
    "# fs_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get selected features for training set\n",
    "# X_train_fs_xgb = pd.DataFrame(rfecv_xgb.transform(X_train))\n",
    "\n",
    "# # rename new dataframe for training set\n",
    "# for i, col in enumerate(X_train_fs_xgb.columns):\n",
    "#     X_train_fs_xgb.rename(columns={col:fs_xgb[i]}, inplace=True)\n",
    "\n",
    "# X_train_fs_xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get selected features for test set\n",
    "# X_test_fs_xgb = pd.DataFrame(rfecv_xgb.transform(X_test))\n",
    "\n",
    "# # rename new dataframe for test set\n",
    "# for i, col in enumerate(X_test_fs_xgb.columns):\n",
    "#     X_test_fs_xgb.rename(columns={col:fs_xgb[i]}, inplace=True)\n",
    "\n",
    "# X_test_fs_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi-square\n",
    "Chi-square is not a good method since we have negative values in our data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from matplotlib.pyplot import figure\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.022109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.100155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.003009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.081279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.015207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2829</th>\n",
       "      <td>0.005137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2830</th>\n",
       "      <td>0.022582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2831</th>\n",
       "      <td>0.002587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2832</th>\n",
       "      <td>0.075648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2833</th>\n",
       "      <td>0.087959</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2834 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0\n",
       "0     0.022109\n",
       "1     0.100155\n",
       "2     0.003009\n",
       "3     0.081279\n",
       "4     0.015207\n",
       "...        ...\n",
       "2829  0.005137\n",
       "2830  0.022582\n",
       "2831  0.002587\n",
       "2832  0.075648\n",
       "2833  0.087959\n",
       "\n",
       "[2834 rows x 1 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mi_score = mutual_info_classif(X_train,y_train, random_state=100)\n",
    "pd.DataFrame(mi_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03544600644160345"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mi_score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 0: 0.022109\n",
      "Feature 1: 0.100155\n",
      "Feature 2: 0.003009\n",
      "Feature 3: 0.081279\n",
      "Feature 4: 0.015207\n",
      "Feature 5: 0.043071\n",
      "Feature 6: 0.153591\n",
      "Feature 7: 0.000000\n",
      "Feature 8: 0.026535\n",
      "Feature 9: 0.089038\n",
      "Feature 10: 0.139435\n",
      "Feature 11: 0.000000\n",
      "Feature 12: 0.000000\n",
      "Feature 13: 0.166814\n",
      "Feature 14: 0.000000\n",
      "Feature 15: 0.077460\n",
      "Feature 16: 0.205600\n",
      "Feature 17: 0.105897\n",
      "Feature 18: 0.089439\n",
      "Feature 19: 0.000000\n",
      "Feature 20: 0.036290\n",
      "Feature 21: 0.071565\n",
      "Feature 22: 0.000000\n",
      "Feature 23: 0.025602\n",
      "Feature 24: 0.076239\n",
      "Feature 25: 0.000000\n",
      "Feature 26: 0.037511\n",
      "Feature 27: 0.000000\n",
      "Feature 28: 0.000000\n",
      "Feature 29: 0.000000\n",
      "Feature 30: 0.000000\n",
      "Feature 31: 0.104604\n",
      "Feature 32: 0.000000\n",
      "Feature 33: 0.055996\n",
      "Feature 34: 0.105985\n",
      "Feature 35: 0.000000\n",
      "Feature 36: 0.000000\n",
      "Feature 37: 0.027161\n",
      "Feature 38: 0.048241\n",
      "Feature 39: 0.105548\n",
      "Feature 40: 0.000000\n",
      "Feature 41: 0.052970\n",
      "Feature 42: 0.051369\n",
      "Feature 43: 0.040241\n",
      "Feature 44: 0.000000\n",
      "Feature 45: 0.000000\n",
      "Feature 46: 0.000000\n",
      "Feature 47: 0.041815\n",
      "Feature 48: 0.005365\n",
      "Feature 49: 0.016527\n",
      "Feature 50: 0.000000\n",
      "Feature 51: 0.000000\n",
      "Feature 52: 0.000000\n",
      "Feature 53: 0.000000\n",
      "Feature 54: 0.106453\n",
      "Feature 55: 0.037283\n",
      "Feature 56: 0.000000\n",
      "Feature 57: 0.000000\n",
      "Feature 58: 0.156949\n",
      "Feature 59: 0.104522\n",
      "Feature 60: 0.095659\n",
      "Feature 61: 0.015456\n",
      "Feature 62: 0.004729\n",
      "Feature 63: 0.061068\n",
      "Feature 64: 0.210424\n",
      "Feature 65: 0.124211\n",
      "Feature 66: 0.047823\n",
      "Feature 67: 0.070279\n",
      "Feature 68: 0.074168\n",
      "Feature 69: 0.110252\n",
      "Feature 70: 0.135850\n",
      "Feature 71: 0.057715\n",
      "Feature 72: 0.059706\n",
      "Feature 73: 0.036311\n",
      "Feature 74: 0.144416\n",
      "Feature 75: 0.126471\n",
      "Feature 76: 0.131028\n",
      "Feature 77: 0.088132\n",
      "Feature 78: 0.056326\n",
      "Feature 79: 0.163192\n",
      "Feature 80: 0.043135\n",
      "Feature 81: 0.179884\n",
      "Feature 82: 0.092601\n",
      "Feature 83: 0.002791\n",
      "Feature 84: 0.015481\n",
      "Feature 85: 0.022490\n",
      "Feature 86: 0.000000\n",
      "Feature 87: 0.015688\n",
      "Feature 88: 0.043409\n",
      "Feature 89: 0.031939\n",
      "Feature 90: 0.033808\n",
      "Feature 91: 0.025213\n",
      "Feature 92: 0.089204\n",
      "Feature 93: 0.053534\n",
      "Feature 94: 0.048876\n",
      "Feature 95: 0.022976\n",
      "Feature 96: 0.000000\n",
      "Feature 97: 0.001320\n",
      "Feature 98: 0.149662\n",
      "Feature 99: 0.141843\n",
      "Feature 100: 0.039496\n",
      "Feature 101: 0.078771\n",
      "Feature 102: 0.000000\n",
      "Feature 103: 0.037677\n",
      "Feature 104: 0.000000\n",
      "Feature 105: 0.000000\n",
      "Feature 106: 0.000000\n",
      "Feature 107: 0.000000\n",
      "Feature 108: 0.081991\n",
      "Feature 109: 0.000000\n",
      "Feature 110: 0.121307\n",
      "Feature 111: 0.006672\n",
      "Feature 112: 0.000000\n",
      "Feature 113: 0.005349\n",
      "Feature 114: 0.000000\n",
      "Feature 115: 0.109275\n",
      "Feature 116: 0.078844\n",
      "Feature 117: 0.000000\n",
      "Feature 118: 0.034022\n",
      "Feature 119: 0.000000\n",
      "Feature 120: 0.028883\n",
      "Feature 121: 0.132793\n",
      "Feature 122: 0.121280\n",
      "Feature 123: 0.039975\n",
      "Feature 124: 0.000000\n",
      "Feature 125: 0.000000\n",
      "Feature 126: 0.000000\n",
      "Feature 127: 0.033879\n",
      "Feature 128: 0.000000\n",
      "Feature 129: 0.038826\n",
      "Feature 130: 0.000000\n",
      "Feature 131: 0.000000\n",
      "Feature 132: 0.070418\n",
      "Feature 133: 0.024515\n",
      "Feature 134: 0.013214\n",
      "Feature 135: 0.097116\n",
      "Feature 136: 0.000000\n",
      "Feature 137: 0.111478\n",
      "Feature 138: 0.095818\n",
      "Feature 139: 0.148828\n",
      "Feature 140: 0.000000\n",
      "Feature 141: 0.000000\n",
      "Feature 142: 0.011018\n",
      "Feature 143: 0.012572\n",
      "Feature 144: 0.017603\n",
      "Feature 145: 0.000000\n",
      "Feature 146: 0.066455\n",
      "Feature 147: 0.057100\n",
      "Feature 148: 0.000000\n",
      "Feature 149: 0.129193\n",
      "Feature 150: 0.052305\n",
      "Feature 151: 0.024262\n",
      "Feature 152: 0.070533\n",
      "Feature 153: 0.020881\n",
      "Feature 154: 0.086966\n",
      "Feature 155: 0.041715\n",
      "Feature 156: 0.016228\n",
      "Feature 157: 0.000000\n",
      "Feature 158: 0.066159\n",
      "Feature 159: 0.020211\n",
      "Feature 160: 0.114338\n",
      "Feature 161: 0.060754\n",
      "Feature 162: 0.000000\n",
      "Feature 163: 0.041750\n",
      "Feature 164: 0.004390\n",
      "Feature 165: 0.000000\n",
      "Feature 166: 0.094591\n",
      "Feature 167: 0.083973\n",
      "Feature 168: 0.029618\n",
      "Feature 169: 0.104917\n",
      "Feature 170: 0.000000\n",
      "Feature 171: 0.050002\n",
      "Feature 172: 0.043136\n",
      "Feature 173: 0.053974\n",
      "Feature 174: 0.000000\n",
      "Feature 175: 0.082458\n",
      "Feature 176: 0.045978\n",
      "Feature 177: 0.132223\n",
      "Feature 178: 0.000000\n",
      "Feature 179: 0.027243\n",
      "Feature 180: 0.010950\n",
      "Feature 181: 0.079854\n",
      "Feature 182: 0.011682\n",
      "Feature 183: 0.000000\n",
      "Feature 184: 0.000000\n",
      "Feature 185: 0.034221\n",
      "Feature 186: 0.000000\n",
      "Feature 187: 0.000000\n",
      "Feature 188: 0.000000\n",
      "Feature 189: 0.000000\n",
      "Feature 190: 0.000000\n",
      "Feature 191: 0.000000\n",
      "Feature 192: 0.046469\n",
      "Feature 193: 0.043286\n",
      "Feature 194: 0.090479\n",
      "Feature 195: 0.000000\n",
      "Feature 196: 0.000000\n",
      "Feature 197: 0.000000\n",
      "Feature 198: 0.013328\n",
      "Feature 199: 0.014532\n",
      "Feature 200: 0.097968\n",
      "Feature 201: 0.034957\n",
      "Feature 202: 0.000000\n",
      "Feature 203: 0.000000\n",
      "Feature 204: 0.000000\n",
      "Feature 205: 0.000000\n",
      "Feature 206: 0.000000\n",
      "Feature 207: 0.011424\n",
      "Feature 208: 0.000000\n",
      "Feature 209: 0.000000\n",
      "Feature 210: 0.043518\n",
      "Feature 211: 0.000000\n",
      "Feature 212: 0.000000\n",
      "Feature 213: 0.000000\n",
      "Feature 214: 0.041290\n",
      "Feature 215: 0.063255\n",
      "Feature 216: 0.000000\n",
      "Feature 217: 0.000000\n",
      "Feature 218: 0.053351\n",
      "Feature 219: 0.034714\n",
      "Feature 220: 0.024897\n",
      "Feature 221: 0.000000\n",
      "Feature 222: 0.000000\n",
      "Feature 223: 0.000000\n",
      "Feature 224: 0.100803\n",
      "Feature 225: 0.058340\n",
      "Feature 226: 0.097244\n",
      "Feature 227: 0.000000\n",
      "Feature 228: 0.000000\n",
      "Feature 229: 0.000000\n",
      "Feature 230: 0.030833\n",
      "Feature 231: 0.007280\n",
      "Feature 232: 0.000000\n",
      "Feature 233: 0.030263\n",
      "Feature 234: 0.000000\n",
      "Feature 235: 0.000000\n",
      "Feature 236: 0.185395\n",
      "Feature 237: 0.000000\n",
      "Feature 238: 0.138214\n",
      "Feature 239: 0.000000\n",
      "Feature 240: 0.047054\n",
      "Feature 241: 0.000000\n",
      "Feature 242: 0.033507\n",
      "Feature 243: 0.033463\n",
      "Feature 244: 0.000000\n",
      "Feature 245: 0.000000\n",
      "Feature 246: 0.000000\n",
      "Feature 247: 0.002493\n",
      "Feature 248: 0.062162\n",
      "Feature 249: 0.000000\n",
      "Feature 250: 0.038395\n",
      "Feature 251: 0.118059\n",
      "Feature 252: 0.000000\n",
      "Feature 253: 0.000000\n",
      "Feature 254: 0.000000\n",
      "Feature 255: 0.000000\n",
      "Feature 256: 0.045363\n",
      "Feature 257: 0.000000\n",
      "Feature 258: 0.000000\n",
      "Feature 259: 0.018453\n",
      "Feature 260: 0.020531\n",
      "Feature 261: 0.000000\n",
      "Feature 262: 0.213828\n",
      "Feature 263: 0.006355\n",
      "Feature 264: 0.103504\n",
      "Feature 265: 0.043351\n",
      "Feature 266: 0.000000\n",
      "Feature 267: 0.000000\n",
      "Feature 268: 0.038434\n",
      "Feature 269: 0.007251\n",
      "Feature 270: 0.000000\n",
      "Feature 271: 0.150410\n",
      "Feature 272: 0.012396\n",
      "Feature 273: 0.066991\n",
      "Feature 274: 0.000000\n",
      "Feature 275: 0.000000\n",
      "Feature 276: 0.000000\n",
      "Feature 277: 0.036473\n",
      "Feature 278: 0.000000\n",
      "Feature 279: 0.003309\n",
      "Feature 280: 0.000000\n",
      "Feature 281: 0.000000\n",
      "Feature 282: 0.000000\n",
      "Feature 283: 0.000000\n",
      "Feature 284: 0.000000\n",
      "Feature 285: 0.000000\n",
      "Feature 286: 0.000000\n",
      "Feature 287: 0.000000\n",
      "Feature 288: 0.018447\n",
      "Feature 289: 0.000000\n",
      "Feature 290: 0.000000\n",
      "Feature 291: 0.040722\n",
      "Feature 292: 0.000000\n",
      "Feature 293: 0.000000\n",
      "Feature 294: 0.000000\n",
      "Feature 295: 0.040684\n",
      "Feature 296: 0.066227\n",
      "Feature 297: 0.113510\n",
      "Feature 298: 0.006437\n",
      "Feature 299: 0.000000\n",
      "Feature 300: 0.000000\n",
      "Feature 301: 0.000000\n",
      "Feature 302: 0.065472\n",
      "Feature 303: 0.000000\n",
      "Feature 304: 0.033225\n",
      "Feature 305: 0.059722\n",
      "Feature 306: 0.000000\n",
      "Feature 307: 0.000000\n",
      "Feature 308: 0.042038\n",
      "Feature 309: 0.000000\n",
      "Feature 310: 0.000000\n",
      "Feature 311: 0.149674\n",
      "Feature 312: 0.064635\n",
      "Feature 313: 0.003522\n",
      "Feature 314: 0.033332\n",
      "Feature 315: 0.077320\n",
      "Feature 316: 0.074850\n",
      "Feature 317: 0.014551\n",
      "Feature 318: 0.009757\n",
      "Feature 319: 0.000000\n",
      "Feature 320: 0.000000\n",
      "Feature 321: 0.007905\n",
      "Feature 322: 0.036239\n",
      "Feature 323: 0.019379\n",
      "Feature 324: 0.017950\n",
      "Feature 325: 0.000000\n",
      "Feature 326: 0.000000\n",
      "Feature 327: 0.000000\n",
      "Feature 328: 0.033757\n",
      "Feature 329: 0.178945\n",
      "Feature 330: 0.000000\n",
      "Feature 331: 0.000000\n",
      "Feature 332: 0.000000\n",
      "Feature 333: 0.058460\n",
      "Feature 334: 0.000000\n",
      "Feature 335: 0.000000\n",
      "Feature 336: 0.024245\n",
      "Feature 337: 0.045605\n",
      "Feature 338: 0.000000\n",
      "Feature 339: 0.000000\n",
      "Feature 340: 0.088307\n",
      "Feature 341: 0.023143\n",
      "Feature 342: 0.064388\n",
      "Feature 343: 0.000000\n",
      "Feature 344: 0.074250\n",
      "Feature 345: 0.000000\n",
      "Feature 346: 0.023256\n",
      "Feature 347: 0.182908\n",
      "Feature 348: 0.000000\n",
      "Feature 349: 0.024745\n",
      "Feature 350: 0.019817\n",
      "Feature 351: 0.016782\n",
      "Feature 352: 0.063065\n",
      "Feature 353: 0.000000\n",
      "Feature 354: 0.079619\n",
      "Feature 355: 0.075753\n",
      "Feature 356: 0.080412\n",
      "Feature 357: 0.000000\n",
      "Feature 358: 0.000000\n",
      "Feature 359: 0.000000\n",
      "Feature 360: 0.005343\n",
      "Feature 361: 0.024843\n",
      "Feature 362: 0.022495\n",
      "Feature 363: 0.130488\n",
      "Feature 364: 0.017621\n",
      "Feature 365: 0.058141\n",
      "Feature 366: 0.000000\n",
      "Feature 367: 0.029684\n",
      "Feature 368: 0.033525\n",
      "Feature 369: 0.000000\n",
      "Feature 370: 0.000000\n",
      "Feature 371: 0.000000\n",
      "Feature 372: 0.000000\n",
      "Feature 373: 0.216451\n",
      "Feature 374: 0.000000\n",
      "Feature 375: 0.074373\n",
      "Feature 376: 0.036619\n",
      "Feature 377: 0.000000\n",
      "Feature 378: 0.155999\n",
      "Feature 379: 0.104928\n",
      "Feature 380: 0.084117\n",
      "Feature 381: 0.000000\n",
      "Feature 382: 0.000000\n",
      "Feature 383: 0.000000\n",
      "Feature 384: 0.000000\n",
      "Feature 385: 0.056566\n",
      "Feature 386: 0.120807\n",
      "Feature 387: 0.069952\n",
      "Feature 388: 0.057781\n",
      "Feature 389: 0.156760\n",
      "Feature 390: 0.000000\n",
      "Feature 391: 0.112344\n",
      "Feature 392: 0.020802\n",
      "Feature 393: 0.071179\n",
      "Feature 394: 0.067500\n",
      "Feature 395: 0.021937\n",
      "Feature 396: 0.065538\n",
      "Feature 397: 0.000000\n",
      "Feature 398: 0.061783\n",
      "Feature 399: 0.000000\n",
      "Feature 400: 0.000000\n",
      "Feature 401: 0.005579\n",
      "Feature 402: 0.056494\n",
      "Feature 403: 0.018411\n",
      "Feature 404: 0.000000\n",
      "Feature 405: 0.000000\n",
      "Feature 406: 0.000000\n",
      "Feature 407: 0.046521\n",
      "Feature 408: 0.000000\n",
      "Feature 409: 0.000000\n",
      "Feature 410: 0.000000\n",
      "Feature 411: 0.000000\n",
      "Feature 412: 0.000000\n",
      "Feature 413: 0.014712\n",
      "Feature 414: 0.000000\n",
      "Feature 415: 0.083635\n",
      "Feature 416: 0.028383\n",
      "Feature 417: 0.000000\n",
      "Feature 418: 0.000000\n",
      "Feature 419: 0.035837\n",
      "Feature 420: 0.000000\n",
      "Feature 421: 0.000000\n",
      "Feature 422: 0.000000\n",
      "Feature 423: 0.000000\n",
      "Feature 424: 0.001204\n",
      "Feature 425: 0.000000\n",
      "Feature 426: 0.000000\n",
      "Feature 427: 0.000000\n",
      "Feature 428: 0.128599\n",
      "Feature 429: 0.000000\n",
      "Feature 430: 0.126138\n",
      "Feature 431: 0.080403\n",
      "Feature 432: 0.000000\n",
      "Feature 433: 0.000000\n",
      "Feature 434: 0.048200\n",
      "Feature 435: 0.000000\n",
      "Feature 436: 0.000000\n",
      "Feature 437: 0.000000\n",
      "Feature 438: 0.030241\n",
      "Feature 439: 0.000000\n",
      "Feature 440: 0.042439\n",
      "Feature 441: 0.000000\n",
      "Feature 442: 0.017417\n",
      "Feature 443: 0.066556\n",
      "Feature 444: 0.131527\n",
      "Feature 445: 0.026420\n",
      "Feature 446: 0.134279\n",
      "Feature 447: 0.147675\n",
      "Feature 448: 0.025556\n",
      "Feature 449: 0.000000\n",
      "Feature 450: 0.050152\n",
      "Feature 451: 0.011559\n",
      "Feature 452: 0.000000\n",
      "Feature 453: 0.052138\n",
      "Feature 454: 0.015645\n",
      "Feature 455: 0.061157\n",
      "Feature 456: 0.110247\n",
      "Feature 457: 0.000000\n",
      "Feature 458: 0.023138\n",
      "Feature 459: 0.076755\n",
      "Feature 460: 0.026786\n",
      "Feature 461: 0.053957\n",
      "Feature 462: 0.102581\n",
      "Feature 463: 0.164341\n",
      "Feature 464: 0.108049\n",
      "Feature 465: 0.041568\n",
      "Feature 466: 0.006904\n",
      "Feature 467: 0.015160\n",
      "Feature 468: 0.103904\n",
      "Feature 469: 0.000000\n",
      "Feature 470: 0.004059\n",
      "Feature 471: 0.030060\n",
      "Feature 472: 0.080592\n",
      "Feature 473: 0.000000\n",
      "Feature 474: 0.076703\n",
      "Feature 475: 0.092420\n",
      "Feature 476: 0.084684\n",
      "Feature 477: 0.099378\n",
      "Feature 478: 0.074826\n",
      "Feature 479: 0.172630\n",
      "Feature 480: 0.157017\n",
      "Feature 481: 0.005604\n",
      "Feature 482: 0.078137\n",
      "Feature 483: 0.177885\n",
      "Feature 484: 0.068654\n",
      "Feature 485: 0.151112\n",
      "Feature 486: 0.032280\n",
      "Feature 487: 0.161986\n",
      "Feature 488: 0.073588\n",
      "Feature 489: 0.183085\n",
      "Feature 490: 0.000000\n",
      "Feature 491: 0.138631\n",
      "Feature 492: 0.040510\n",
      "Feature 493: 0.100603\n",
      "Feature 494: 0.032999\n",
      "Feature 495: 0.000000\n",
      "Feature 496: 0.000000\n",
      "Feature 497: 0.092474\n",
      "Feature 498: 0.114132\n",
      "Feature 499: 0.009637\n",
      "Feature 500: 0.126690\n",
      "Feature 501: 0.000000\n",
      "Feature 502: 0.002473\n",
      "Feature 503: 0.000000\n",
      "Feature 504: 0.013624\n",
      "Feature 505: 0.037644\n",
      "Feature 506: 0.154361\n",
      "Feature 507: 0.000000\n",
      "Feature 508: 0.000000\n",
      "Feature 509: 0.000000\n",
      "Feature 510: 0.095429\n",
      "Feature 511: 0.000000\n",
      "Feature 512: 0.000000\n",
      "Feature 513: 0.000000\n",
      "Feature 514: 0.051191\n",
      "Feature 515: 0.077098\n",
      "Feature 516: 0.000000\n",
      "Feature 517: 0.073121\n",
      "Feature 518: 0.043096\n",
      "Feature 519: 0.041771\n",
      "Feature 520: 0.108659\n",
      "Feature 521: 0.000000\n",
      "Feature 522: 0.000000\n",
      "Feature 523: 0.003869\n",
      "Feature 524: 0.000000\n",
      "Feature 525: 0.000000\n",
      "Feature 526: 0.000000\n",
      "Feature 527: 0.005174\n",
      "Feature 528: 0.000000\n",
      "Feature 529: 0.000000\n",
      "Feature 530: 0.075664\n",
      "Feature 531: 0.000000\n",
      "Feature 532: 0.074294\n",
      "Feature 533: 0.000000\n",
      "Feature 534: 0.000000\n",
      "Feature 535: 0.000000\n",
      "Feature 536: 0.000000\n",
      "Feature 537: 0.048766\n",
      "Feature 538: 0.000000\n",
      "Feature 539: 0.128610\n",
      "Feature 540: 0.002057\n",
      "Feature 541: 0.055632\n",
      "Feature 542: 0.000000\n",
      "Feature 543: 0.070294\n",
      "Feature 544: 0.000000\n",
      "Feature 545: 0.000000\n",
      "Feature 546: 0.037579\n",
      "Feature 547: 0.000000\n",
      "Feature 548: 0.016320\n",
      "Feature 549: 0.000000\n",
      "Feature 550: 0.036392\n",
      "Feature 551: 0.037089\n",
      "Feature 552: 0.023387\n",
      "Feature 553: 0.042147\n",
      "Feature 554: 0.166288\n",
      "Feature 555: 0.000000\n",
      "Feature 556: 0.023393\n",
      "Feature 557: 0.056180\n",
      "Feature 558: 0.000000\n",
      "Feature 559: 0.034905\n",
      "Feature 560: 0.000000\n",
      "Feature 561: 0.000000\n",
      "Feature 562: 0.000000\n",
      "Feature 563: 0.000000\n",
      "Feature 564: 0.000000\n",
      "Feature 565: 0.082224\n",
      "Feature 566: 0.000000\n",
      "Feature 567: 0.000000\n",
      "Feature 568: 0.000000\n",
      "Feature 569: 0.063661\n",
      "Feature 570: 0.000000\n",
      "Feature 571: 0.000000\n",
      "Feature 572: 0.000000\n",
      "Feature 573: 0.000000\n",
      "Feature 574: 0.000000\n",
      "Feature 575: 0.080747\n",
      "Feature 576: 0.000000\n",
      "Feature 577: 0.000000\n",
      "Feature 578: 0.003791\n",
      "Feature 579: 0.020854\n",
      "Feature 580: 0.000000\n",
      "Feature 581: 0.000000\n",
      "Feature 582: 0.000000\n",
      "Feature 583: 0.000000\n",
      "Feature 584: 0.000000\n",
      "Feature 585: 0.000000\n",
      "Feature 586: 0.000000\n",
      "Feature 587: 0.000000\n",
      "Feature 588: 0.042378\n",
      "Feature 589: 0.000000\n",
      "Feature 590: 0.000000\n",
      "Feature 591: 0.000000\n",
      "Feature 592: 0.044044\n",
      "Feature 593: 0.097387\n",
      "Feature 594: 0.000000\n",
      "Feature 595: 0.000000\n",
      "Feature 596: 0.105042\n",
      "Feature 597: 0.000000\n",
      "Feature 598: 0.000000\n",
      "Feature 599: 0.000000\n",
      "Feature 600: 0.011156\n",
      "Feature 601: 0.031420\n",
      "Feature 602: 0.024770\n",
      "Feature 603: 0.000000\n",
      "Feature 604: 0.000000\n",
      "Feature 605: 0.000000\n",
      "Feature 606: 0.000000\n",
      "Feature 607: 0.011987\n",
      "Feature 608: 0.095635\n",
      "Feature 609: 0.068711\n",
      "Feature 610: 0.019343\n",
      "Feature 611: 0.153720\n",
      "Feature 612: 0.000000\n",
      "Feature 613: 0.070361\n",
      "Feature 614: 0.000000\n",
      "Feature 615: 0.122007\n",
      "Feature 616: 0.035915\n",
      "Feature 617: 0.088017\n",
      "Feature 618: 0.083442\n",
      "Feature 619: 0.040508\n",
      "Feature 620: 0.187950\n",
      "Feature 621: 0.124494\n",
      "Feature 622: 0.000000\n",
      "Feature 623: 0.083862\n",
      "Feature 624: 0.000000\n",
      "Feature 625: 0.116963\n",
      "Feature 626: 0.000000\n",
      "Feature 627: 0.238317\n",
      "Feature 628: 0.000000\n",
      "Feature 629: 0.083249\n",
      "Feature 630: 0.006590\n",
      "Feature 631: 0.000000\n",
      "Feature 632: 0.000000\n",
      "Feature 633: 0.000000\n",
      "Feature 634: 0.000000\n",
      "Feature 635: 0.015937\n",
      "Feature 636: 0.074757\n",
      "Feature 637: 0.000000\n",
      "Feature 638: 0.137937\n",
      "Feature 639: 0.064681\n",
      "Feature 640: 0.059695\n",
      "Feature 641: 0.039928\n",
      "Feature 642: 0.000000\n",
      "Feature 643: 0.080671\n",
      "Feature 644: 0.000000\n",
      "Feature 645: 0.002521\n",
      "Feature 646: 0.000000\n",
      "Feature 647: 0.124815\n",
      "Feature 648: 0.000000\n",
      "Feature 649: 0.001540\n",
      "Feature 650: 0.020431\n",
      "Feature 651: 0.000000\n",
      "Feature 652: 0.000000\n",
      "Feature 653: 0.000000\n",
      "Feature 654: 0.053452\n",
      "Feature 655: 0.000000\n",
      "Feature 656: 0.104017\n",
      "Feature 657: 0.000000\n",
      "Feature 658: 0.000000\n",
      "Feature 659: 0.054777\n",
      "Feature 660: 0.000000\n",
      "Feature 661: 0.006179\n",
      "Feature 662: 0.000000\n",
      "Feature 663: 0.045348\n",
      "Feature 664: 0.049374\n",
      "Feature 665: 0.000000\n",
      "Feature 666: 0.000000\n",
      "Feature 667: 0.000000\n",
      "Feature 668: 0.083632\n",
      "Feature 669: 0.072734\n",
      "Feature 670: 0.000000\n",
      "Feature 671: 0.025066\n",
      "Feature 672: 0.044097\n",
      "Feature 673: 0.066738\n",
      "Feature 674: 0.000000\n",
      "Feature 675: 0.044277\n",
      "Feature 676: 0.064602\n",
      "Feature 677: 0.050698\n",
      "Feature 678: 0.000000\n",
      "Feature 679: 0.050530\n",
      "Feature 680: 0.000000\n",
      "Feature 681: 0.000000\n",
      "Feature 682: 0.147648\n",
      "Feature 683: 0.048609\n",
      "Feature 684: 0.095993\n",
      "Feature 685: 0.000000\n",
      "Feature 686: 0.000000\n",
      "Feature 687: 0.047482\n",
      "Feature 688: 0.062177\n",
      "Feature 689: 0.068239\n",
      "Feature 690: 0.082358\n",
      "Feature 691: 0.184944\n",
      "Feature 692: 0.095183\n",
      "Feature 693: 0.151636\n",
      "Feature 694: 0.238038\n",
      "Feature 695: 0.000000\n",
      "Feature 696: 0.023053\n",
      "Feature 697: 0.097455\n",
      "Feature 698: 0.000000\n",
      "Feature 699: 0.000000\n",
      "Feature 700: 0.060424\n",
      "Feature 701: 0.000000\n",
      "Feature 702: 0.000000\n",
      "Feature 703: 0.022962\n",
      "Feature 704: 0.009428\n",
      "Feature 705: 0.000000\n",
      "Feature 706: 0.000000\n",
      "Feature 707: 0.022505\n",
      "Feature 708: 0.093386\n",
      "Feature 709: 0.000000\n",
      "Feature 710: 0.000000\n",
      "Feature 711: 0.091611\n",
      "Feature 712: 0.016722\n",
      "Feature 713: 0.000000\n",
      "Feature 714: 0.000000\n",
      "Feature 715: 0.036549\n",
      "Feature 716: 0.058696\n",
      "Feature 717: 0.004960\n",
      "Feature 718: 0.045408\n",
      "Feature 719: 0.006111\n",
      "Feature 720: 0.044303\n",
      "Feature 721: 0.038787\n",
      "Feature 722: 0.000000\n",
      "Feature 723: 0.058870\n",
      "Feature 724: 0.066111\n",
      "Feature 725: 0.000000\n",
      "Feature 726: 0.052618\n",
      "Feature 727: 0.068172\n",
      "Feature 728: 0.074477\n",
      "Feature 729: 0.040299\n",
      "Feature 730: 0.138222\n",
      "Feature 731: 0.000000\n",
      "Feature 732: 0.139863\n",
      "Feature 733: 0.000000\n",
      "Feature 734: 0.127810\n",
      "Feature 735: 0.086515\n",
      "Feature 736: 0.113750\n",
      "Feature 737: 0.000000\n",
      "Feature 738: 0.033170\n",
      "Feature 739: 0.017635\n",
      "Feature 740: 0.046526\n",
      "Feature 741: 0.000000\n",
      "Feature 742: 0.027277\n",
      "Feature 743: 0.170705\n",
      "Feature 744: 0.080971\n",
      "Feature 745: 0.008636\n",
      "Feature 746: 0.097207\n",
      "Feature 747: 0.000000\n",
      "Feature 748: 0.045016\n",
      "Feature 749: 0.068422\n",
      "Feature 750: 0.056765\n",
      "Feature 751: 0.034399\n",
      "Feature 752: 0.082394\n",
      "Feature 753: 0.000000\n",
      "Feature 754: 0.077358\n",
      "Feature 755: 0.045980\n",
      "Feature 756: 0.000000\n",
      "Feature 757: 0.000000\n",
      "Feature 758: 0.028886\n",
      "Feature 759: 0.042880\n",
      "Feature 760: 0.000000\n",
      "Feature 761: 0.000000\n",
      "Feature 762: 0.000000\n",
      "Feature 763: 0.000000\n",
      "Feature 764: 0.070454\n",
      "Feature 765: 0.000000\n",
      "Feature 766: 0.092486\n",
      "Feature 767: 0.000000\n",
      "Feature 768: 0.067402\n",
      "Feature 769: 0.000000\n",
      "Feature 770: 0.000000\n",
      "Feature 771: 0.114975\n",
      "Feature 772: 0.159352\n",
      "Feature 773: 0.000000\n",
      "Feature 774: 0.000000\n",
      "Feature 775: 0.065953\n",
      "Feature 776: 0.019455\n",
      "Feature 777: 0.000000\n",
      "Feature 778: 0.021226\n",
      "Feature 779: 0.000000\n",
      "Feature 780: 0.076965\n",
      "Feature 781: 0.033540\n",
      "Feature 782: 0.000000\n",
      "Feature 783: 0.103532\n",
      "Feature 784: 0.014549\n",
      "Feature 785: 0.009135\n",
      "Feature 786: 0.030216\n",
      "Feature 787: 0.000000\n",
      "Feature 788: 0.000000\n",
      "Feature 789: 0.138275\n",
      "Feature 790: 0.000000\n",
      "Feature 791: 0.060399\n",
      "Feature 792: 0.022487\n",
      "Feature 793: 0.091463\n",
      "Feature 794: 0.000000\n",
      "Feature 795: 0.000000\n",
      "Feature 796: 0.000000\n",
      "Feature 797: 0.000000\n",
      "Feature 798: 0.056434\n",
      "Feature 799: 0.034345\n",
      "Feature 800: 0.000000\n",
      "Feature 801: 0.005302\n",
      "Feature 802: 0.030105\n",
      "Feature 803: 0.000000\n",
      "Feature 804: 0.121175\n",
      "Feature 805: 0.000000\n",
      "Feature 806: 0.000000\n",
      "Feature 807: 0.000000\n",
      "Feature 808: 0.084134\n",
      "Feature 809: 0.045578\n",
      "Feature 810: 0.014564\n",
      "Feature 811: 0.000000\n",
      "Feature 812: 0.000000\n",
      "Feature 813: 0.041697\n",
      "Feature 814: 0.128874\n",
      "Feature 815: 0.000000\n",
      "Feature 816: 0.039349\n",
      "Feature 817: 0.000000\n",
      "Feature 818: 0.009586\n",
      "Feature 819: 0.069058\n",
      "Feature 820: 0.000000\n",
      "Feature 821: 0.000000\n",
      "Feature 822: 0.023395\n",
      "Feature 823: 0.123360\n",
      "Feature 824: 0.000000\n",
      "Feature 825: 0.026026\n",
      "Feature 826: 0.015816\n",
      "Feature 827: 0.000000\n",
      "Feature 828: 0.055924\n",
      "Feature 829: 0.056995\n",
      "Feature 830: 0.192195\n",
      "Feature 831: 0.174943\n",
      "Feature 832: 0.034297\n",
      "Feature 833: 0.168300\n",
      "Feature 834: 0.025247\n",
      "Feature 835: 0.033482\n",
      "Feature 836: 0.043772\n",
      "Feature 837: 0.127129\n",
      "Feature 838: 0.024250\n",
      "Feature 839: 0.131306\n",
      "Feature 840: 0.122372\n",
      "Feature 841: 0.145425\n",
      "Feature 842: 0.110228\n",
      "Feature 843: 0.160641\n",
      "Feature 844: 0.050318\n",
      "Feature 845: 0.049294\n",
      "Feature 846: 0.000000\n",
      "Feature 847: 0.113023\n",
      "Feature 848: 0.105910\n",
      "Feature 849: 0.083100\n",
      "Feature 850: 0.134417\n",
      "Feature 851: 0.105217\n",
      "Feature 852: 0.090130\n",
      "Feature 853: 0.173685\n",
      "Feature 854: 0.128916\n",
      "Feature 855: 0.170450\n",
      "Feature 856: 0.097454\n",
      "Feature 857: 0.090956\n",
      "Feature 858: 0.137827\n",
      "Feature 859: 0.098366\n",
      "Feature 860: 0.036045\n",
      "Feature 861: 0.062694\n",
      "Feature 862: 0.059759\n",
      "Feature 863: 0.086289\n",
      "Feature 864: 0.056406\n",
      "Feature 865: 0.074366\n",
      "Feature 866: 0.000000\n",
      "Feature 867: 0.053836\n",
      "Feature 868: 0.081255\n",
      "Feature 869: 0.037649\n",
      "Feature 870: 0.046389\n",
      "Feature 871: 0.035306\n",
      "Feature 872: 0.000000\n",
      "Feature 873: 0.038812\n",
      "Feature 874: 0.039950\n",
      "Feature 875: 0.032459\n",
      "Feature 876: 0.000000\n",
      "Feature 877: 0.145523\n",
      "Feature 878: 0.000000\n",
      "Feature 879: 0.000000\n",
      "Feature 880: 0.140057\n",
      "Feature 881: 0.013910\n",
      "Feature 882: 0.008524\n",
      "Feature 883: 0.004859\n",
      "Feature 884: 0.000000\n",
      "Feature 885: 0.000000\n",
      "Feature 886: 0.033745\n",
      "Feature 887: 0.000000\n",
      "Feature 888: 0.071398\n",
      "Feature 889: 0.097175\n",
      "Feature 890: 0.000000\n",
      "Feature 891: 0.134158\n",
      "Feature 892: 0.000000\n",
      "Feature 893: 0.001787\n",
      "Feature 894: 0.044824\n",
      "Feature 895: 0.000000\n",
      "Feature 896: 0.028543\n",
      "Feature 897: 0.071105\n",
      "Feature 898: 0.000000\n",
      "Feature 899: 0.010411\n",
      "Feature 900: 0.000000\n",
      "Feature 901: 0.018469\n",
      "Feature 902: 0.059093\n",
      "Feature 903: 0.103131\n",
      "Feature 904: 0.114884\n",
      "Feature 905: 0.047730\n",
      "Feature 906: 0.000000\n",
      "Feature 907: 0.101505\n",
      "Feature 908: 0.031398\n",
      "Feature 909: 0.055180\n",
      "Feature 910: 0.124306\n",
      "Feature 911: 0.136385\n",
      "Feature 912: 0.054835\n",
      "Feature 913: 0.010807\n",
      "Feature 914: 0.121392\n",
      "Feature 915: 0.124452\n",
      "Feature 916: 0.061802\n",
      "Feature 917: 0.000000\n",
      "Feature 918: 0.048023\n",
      "Feature 919: 0.112444\n",
      "Feature 920: 0.000000\n",
      "Feature 921: 0.000000\n",
      "Feature 922: 0.002625\n",
      "Feature 923: 0.000000\n",
      "Feature 924: 0.009388\n",
      "Feature 925: 0.042307\n",
      "Feature 926: 0.000000\n",
      "Feature 927: 0.000000\n",
      "Feature 928: 0.000000\n",
      "Feature 929: 0.000000\n",
      "Feature 930: 0.000000\n",
      "Feature 931: 0.000000\n",
      "Feature 932: 0.023483\n",
      "Feature 933: 0.047539\n",
      "Feature 934: 0.000000\n",
      "Feature 935: 0.000000\n",
      "Feature 936: 0.044480\n",
      "Feature 937: 0.009001\n",
      "Feature 938: 0.000000\n",
      "Feature 939: 0.076204\n",
      "Feature 940: 0.007878\n",
      "Feature 941: 0.000000\n",
      "Feature 942: 0.000000\n",
      "Feature 943: 0.000000\n",
      "Feature 944: 0.066252\n",
      "Feature 945: 0.000000\n",
      "Feature 946: 0.000000\n",
      "Feature 947: 0.013407\n",
      "Feature 948: 0.000000\n",
      "Feature 949: 0.000000\n",
      "Feature 950: 0.000000\n",
      "Feature 951: 0.000000\n",
      "Feature 952: 0.002118\n",
      "Feature 953: 0.051173\n",
      "Feature 954: 0.000000\n",
      "Feature 955: 0.000000\n",
      "Feature 956: 0.000000\n",
      "Feature 957: 0.114074\n",
      "Feature 958: 0.041415\n",
      "Feature 959: 0.000000\n",
      "Feature 960: 0.083144\n",
      "Feature 961: 0.000000\n",
      "Feature 962: 0.069099\n",
      "Feature 963: 0.000000\n",
      "Feature 964: 0.000000\n",
      "Feature 965: 0.000000\n",
      "Feature 966: 0.001892\n",
      "Feature 967: 0.000000\n",
      "Feature 968: 0.005997\n",
      "Feature 969: 0.000000\n",
      "Feature 970: 0.023668\n",
      "Feature 971: 0.000000\n",
      "Feature 972: 0.000000\n",
      "Feature 973: 0.010029\n",
      "Feature 974: 0.000000\n",
      "Feature 975: 0.000000\n",
      "Feature 976: 0.030154\n",
      "Feature 977: 0.081659\n",
      "Feature 978: 0.061040\n",
      "Feature 979: 0.000000\n",
      "Feature 980: 0.000000\n",
      "Feature 981: 0.000000\n",
      "Feature 982: 0.001079\n",
      "Feature 983: 0.101287\n",
      "Feature 984: 0.000000\n",
      "Feature 985: 0.000000\n",
      "Feature 986: 0.000000\n",
      "Feature 987: 0.064459\n",
      "Feature 988: 0.026350\n",
      "Feature 989: 0.029457\n",
      "Feature 990: 0.000000\n",
      "Feature 991: 0.000000\n",
      "Feature 992: 0.129610\n",
      "Feature 993: 0.000000\n",
      "Feature 994: 0.050069\n",
      "Feature 995: 0.000000\n",
      "Feature 996: 0.043111\n",
      "Feature 997: 0.000000\n",
      "Feature 998: 0.044428\n",
      "Feature 999: 0.171885\n",
      "Feature 1000: 0.146495\n",
      "Feature 1001: 0.180920\n",
      "Feature 1002: 0.049834\n",
      "Feature 1003: 0.065040\n",
      "Feature 1004: 0.124475\n",
      "Feature 1005: 0.064300\n",
      "Feature 1006: 0.087841\n",
      "Feature 1007: 0.023597\n",
      "Feature 1008: 0.057442\n",
      "Feature 1009: 0.000000\n",
      "Feature 1010: 0.000000\n",
      "Feature 1011: 0.000000\n",
      "Feature 1012: 0.000000\n",
      "Feature 1013: 0.000000\n",
      "Feature 1014: 0.108657\n",
      "Feature 1015: 0.090143\n",
      "Feature 1016: 0.164046\n",
      "Feature 1017: 0.045148\n",
      "Feature 1018: 0.000750\n",
      "Feature 1019: 0.021512\n",
      "Feature 1020: 0.042083\n",
      "Feature 1021: 0.000000\n",
      "Feature 1022: 0.012182\n",
      "Feature 1023: 0.000000\n",
      "Feature 1024: 0.029055\n",
      "Feature 1025: 0.066216\n",
      "Feature 1026: 0.050521\n",
      "Feature 1027: 0.123804\n",
      "Feature 1028: 0.012785\n",
      "Feature 1029: 0.064308\n",
      "Feature 1030: 0.000000\n",
      "Feature 1031: 0.000000\n",
      "Feature 1032: 0.000000\n",
      "Feature 1033: 0.035861\n",
      "Feature 1034: 0.003622\n",
      "Feature 1035: 0.048298\n",
      "Feature 1036: 0.000000\n",
      "Feature 1037: 0.050614\n",
      "Feature 1038: 0.082550\n",
      "Feature 1039: 0.000000\n",
      "Feature 1040: 0.000000\n",
      "Feature 1041: 0.028298\n",
      "Feature 1042: 0.000000\n",
      "Feature 1043: 0.129810\n",
      "Feature 1044: 0.009264\n",
      "Feature 1045: 0.000000\n",
      "Feature 1046: 0.000000\n",
      "Feature 1047: 0.041107\n",
      "Feature 1048: 0.000000\n",
      "Feature 1049: 0.000000\n",
      "Feature 1050: 0.081624\n",
      "Feature 1051: 0.000000\n",
      "Feature 1052: 0.000000\n",
      "Feature 1053: 0.028535\n",
      "Feature 1054: 0.000642\n",
      "Feature 1055: 0.138144\n",
      "Feature 1056: 0.038963\n",
      "Feature 1057: 0.018094\n",
      "Feature 1058: 0.046111\n",
      "Feature 1059: 0.002787\n",
      "Feature 1060: 0.084135\n",
      "Feature 1061: 0.071640\n",
      "Feature 1062: 0.089364\n",
      "Feature 1063: 0.000000\n",
      "Feature 1064: 0.077954\n",
      "Feature 1065: 0.058834\n",
      "Feature 1066: 0.000000\n",
      "Feature 1067: 0.024695\n",
      "Feature 1068: 0.049679\n",
      "Feature 1069: 0.022339\n",
      "Feature 1070: 0.000000\n",
      "Feature 1071: 0.000000\n",
      "Feature 1072: 0.053367\n",
      "Feature 1073: 0.000000\n",
      "Feature 1074: 0.000000\n",
      "Feature 1075: 0.000000\n",
      "Feature 1076: 0.105402\n",
      "Feature 1077: 0.015990\n",
      "Feature 1078: 0.041790\n",
      "Feature 1079: 0.034617\n",
      "Feature 1080: 0.000000\n",
      "Feature 1081: 0.107661\n",
      "Feature 1082: 0.017275\n",
      "Feature 1083: 0.047768\n",
      "Feature 1084: 0.144629\n",
      "Feature 1085: 0.127707\n",
      "Feature 1086: 0.107000\n",
      "Feature 1087: 0.039236\n",
      "Feature 1088: 0.101931\n",
      "Feature 1089: 0.074177\n",
      "Feature 1090: 0.085957\n",
      "Feature 1091: 0.012709\n",
      "Feature 1092: 0.044768\n",
      "Feature 1093: 0.000000\n",
      "Feature 1094: 0.090293\n",
      "Feature 1095: 0.000000\n",
      "Feature 1096: 0.012933\n",
      "Feature 1097: 0.000000\n",
      "Feature 1098: 0.000000\n",
      "Feature 1099: 0.000000\n",
      "Feature 1100: 0.000000\n",
      "Feature 1101: 0.006910\n",
      "Feature 1102: 0.065603\n",
      "Feature 1103: 0.099898\n",
      "Feature 1104: 0.109672\n",
      "Feature 1105: 0.054853\n",
      "Feature 1106: 0.000000\n",
      "Feature 1107: 0.072370\n",
      "Feature 1108: 0.116704\n",
      "Feature 1109: 0.000000\n",
      "Feature 1110: 0.107634\n",
      "Feature 1111: 0.063543\n",
      "Feature 1112: 0.145844\n",
      "Feature 1113: 0.000000\n",
      "Feature 1114: 0.019863\n",
      "Feature 1115: 0.015902\n",
      "Feature 1116: 0.000000\n",
      "Feature 1117: 0.000000\n",
      "Feature 1118: 0.057229\n",
      "Feature 1119: 0.062044\n",
      "Feature 1120: 0.011812\n",
      "Feature 1121: 0.092347\n",
      "Feature 1122: 0.038793\n",
      "Feature 1123: 0.243697\n",
      "Feature 1124: 0.036308\n",
      "Feature 1125: 0.019049\n",
      "Feature 1126: 0.092417\n",
      "Feature 1127: 0.047535\n",
      "Feature 1128: 0.000000\n",
      "Feature 1129: 0.000000\n",
      "Feature 1130: 0.027520\n",
      "Feature 1131: 0.000000\n",
      "Feature 1132: 0.000000\n",
      "Feature 1133: 0.011015\n",
      "Feature 1134: 0.045082\n",
      "Feature 1135: 0.000000\n",
      "Feature 1136: 0.044872\n",
      "Feature 1137: 0.029446\n",
      "Feature 1138: 0.047771\n",
      "Feature 1139: 0.033712\n",
      "Feature 1140: 0.000000\n",
      "Feature 1141: 0.067966\n",
      "Feature 1142: 0.019270\n",
      "Feature 1143: 0.128097\n",
      "Feature 1144: 0.000000\n",
      "Feature 1145: 0.000000\n",
      "Feature 1146: 0.049020\n",
      "Feature 1147: 0.000000\n",
      "Feature 1148: 0.045964\n",
      "Feature 1149: 0.000000\n",
      "Feature 1150: 0.014111\n",
      "Feature 1151: 0.021763\n",
      "Feature 1152: 0.005204\n",
      "Feature 1153: 0.000000\n",
      "Feature 1154: 0.000000\n",
      "Feature 1155: 0.023985\n",
      "Feature 1156: 0.000000\n",
      "Feature 1157: 0.000000\n",
      "Feature 1158: 0.000000\n",
      "Feature 1159: 0.000000\n",
      "Feature 1160: 0.000000\n",
      "Feature 1161: 0.000000\n",
      "Feature 1162: 0.098005\n",
      "Feature 1163: 0.074330\n",
      "Feature 1164: 0.061679\n",
      "Feature 1165: 0.062040\n",
      "Feature 1166: 0.000000\n",
      "Feature 1167: 0.040615\n",
      "Feature 1168: 0.005365\n",
      "Feature 1169: 0.060214\n",
      "Feature 1170: 0.065868\n",
      "Feature 1171: 0.069352\n",
      "Feature 1172: 0.000000\n",
      "Feature 1173: 0.000000\n",
      "Feature 1174: 0.066683\n",
      "Feature 1175: 0.000000\n",
      "Feature 1176: 0.001176\n",
      "Feature 1177: 0.055460\n",
      "Feature 1178: 0.000000\n",
      "Feature 1179: 0.000000\n",
      "Feature 1180: 0.034254\n",
      "Feature 1181: 0.074971\n",
      "Feature 1182: 0.047361\n",
      "Feature 1183: 0.073722\n",
      "Feature 1184: 0.000000\n",
      "Feature 1185: 0.000000\n",
      "Feature 1186: 0.000000\n",
      "Feature 1187: 0.000000\n",
      "Feature 1188: 0.003155\n",
      "Feature 1189: 0.042869\n",
      "Feature 1190: 0.060618\n",
      "Feature 1191: 0.000000\n",
      "Feature 1192: 0.113640\n",
      "Feature 1193: 0.000000\n",
      "Feature 1194: 0.000000\n",
      "Feature 1195: 0.012414\n",
      "Feature 1196: 0.000000\n",
      "Feature 1197: 0.000000\n",
      "Feature 1198: 0.075513\n",
      "Feature 1199: 0.127537\n",
      "Feature 1200: 0.000000\n",
      "Feature 1201: 0.021035\n",
      "Feature 1202: 0.000000\n",
      "Feature 1203: 0.022001\n",
      "Feature 1204: 0.000000\n",
      "Feature 1205: 0.000000\n",
      "Feature 1206: 0.000000\n",
      "Feature 1207: 0.000000\n",
      "Feature 1208: 0.049825\n",
      "Feature 1209: 0.027293\n",
      "Feature 1210: 0.000000\n",
      "Feature 1211: 0.000000\n",
      "Feature 1212: 0.000000\n",
      "Feature 1213: 0.000000\n",
      "Feature 1214: 0.021621\n",
      "Feature 1215: 0.104847\n",
      "Feature 1216: 0.000000\n",
      "Feature 1217: 0.000000\n",
      "Feature 1218: 0.055161\n",
      "Feature 1219: 0.024593\n",
      "Feature 1220: 0.000000\n",
      "Feature 1221: 0.000000\n",
      "Feature 1222: 0.161268\n",
      "Feature 1223: 0.013625\n",
      "Feature 1224: 0.000000\n",
      "Feature 1225: 0.000000\n",
      "Feature 1226: 0.174852\n",
      "Feature 1227: 0.018189\n",
      "Feature 1228: 0.025573\n",
      "Feature 1229: 0.000000\n",
      "Feature 1230: 0.140518\n",
      "Feature 1231: 0.000000\n",
      "Feature 1232: 0.005478\n",
      "Feature 1233: 0.042435\n",
      "Feature 1234: 0.058639\n",
      "Feature 1235: 0.122763\n",
      "Feature 1236: 0.000000\n",
      "Feature 1237: 0.030960\n",
      "Feature 1238: 0.089124\n",
      "Feature 1239: 0.047417\n",
      "Feature 1240: 0.017133\n",
      "Feature 1241: 0.000000\n",
      "Feature 1242: 0.000000\n",
      "Feature 1243: 0.059990\n",
      "Feature 1244: 0.000000\n",
      "Feature 1245: 0.000000\n",
      "Feature 1246: 0.000000\n",
      "Feature 1247: 0.000000\n",
      "Feature 1248: 0.056092\n",
      "Feature 1249: 0.000000\n",
      "Feature 1250: 0.174966\n",
      "Feature 1251: 0.128133\n",
      "Feature 1252: 0.000000\n",
      "Feature 1253: 0.012647\n",
      "Feature 1254: 0.000000\n",
      "Feature 1255: 0.000000\n",
      "Feature 1256: 0.123152\n",
      "Feature 1257: 0.078231\n",
      "Feature 1258: 0.000000\n",
      "Feature 1259: 0.000000\n",
      "Feature 1260: 0.000000\n",
      "Feature 1261: 0.004522\n",
      "Feature 1262: 0.000000\n",
      "Feature 1263: 0.000000\n",
      "Feature 1264: 0.000000\n",
      "Feature 1265: 0.033044\n",
      "Feature 1266: 0.000000\n",
      "Feature 1267: 0.000000\n",
      "Feature 1268: 0.006176\n",
      "Feature 1269: 0.000000\n",
      "Feature 1270: 0.024986\n",
      "Feature 1271: 0.000000\n",
      "Feature 1272: 0.014898\n",
      "Feature 1273: 0.000000\n",
      "Feature 1274: 0.054951\n",
      "Feature 1275: 0.018349\n",
      "Feature 1276: 0.042044\n",
      "Feature 1277: 0.034417\n",
      "Feature 1278: 0.046787\n",
      "Feature 1279: 0.051569\n",
      "Feature 1280: 0.000000\n",
      "Feature 1281: 0.077690\n",
      "Feature 1282: 0.039590\n",
      "Feature 1283: 0.000000\n",
      "Feature 1284: 0.016275\n",
      "Feature 1285: 0.000000\n",
      "Feature 1286: 0.044998\n",
      "Feature 1287: 0.036899\n",
      "Feature 1288: 0.004798\n",
      "Feature 1289: 0.000000\n",
      "Feature 1290: 0.098921\n",
      "Feature 1291: 0.000000\n",
      "Feature 1292: 0.000000\n",
      "Feature 1293: 0.000000\n",
      "Feature 1294: 0.005554\n",
      "Feature 1295: 0.121444\n",
      "Feature 1296: 0.000000\n",
      "Feature 1297: 0.000000\n",
      "Feature 1298: 0.031718\n",
      "Feature 1299: 0.002180\n",
      "Feature 1300: 0.069147\n",
      "Feature 1301: 0.176539\n",
      "Feature 1302: 0.080946\n",
      "Feature 1303: 0.000000\n",
      "Feature 1304: 0.131712\n",
      "Feature 1305: 0.047651\n",
      "Feature 1306: 0.072434\n",
      "Feature 1307: 0.043131\n",
      "Feature 1308: 0.000000\n",
      "Feature 1309: 0.004739\n",
      "Feature 1310: 0.015692\n",
      "Feature 1311: 0.078734\n",
      "Feature 1312: 0.018310\n",
      "Feature 1313: 0.047296\n",
      "Feature 1314: 0.000000\n",
      "Feature 1315: 0.035280\n",
      "Feature 1316: 0.028956\n",
      "Feature 1317: 0.004015\n",
      "Feature 1318: 0.013890\n",
      "Feature 1319: 0.000000\n",
      "Feature 1320: 0.000000\n",
      "Feature 1321: 0.033861\n",
      "Feature 1322: 0.080236\n",
      "Feature 1323: 0.013294\n",
      "Feature 1324: 0.049374\n",
      "Feature 1325: 0.000000\n",
      "Feature 1326: 0.000000\n",
      "Feature 1327: 0.042568\n",
      "Feature 1328: 0.000000\n",
      "Feature 1329: 0.000000\n",
      "Feature 1330: 0.000000\n",
      "Feature 1331: 0.000000\n",
      "Feature 1332: 0.089940\n",
      "Feature 1333: 0.000000\n",
      "Feature 1334: 0.034218\n",
      "Feature 1335: 0.000000\n",
      "Feature 1336: 0.000000\n",
      "Feature 1337: 0.000000\n",
      "Feature 1338: 0.045336\n",
      "Feature 1339: 0.012575\n",
      "Feature 1340: 0.012154\n",
      "Feature 1341: 0.069158\n",
      "Feature 1342: 0.000000\n",
      "Feature 1343: 0.024242\n",
      "Feature 1344: 0.000000\n",
      "Feature 1345: 0.007823\n",
      "Feature 1346: 0.007269\n",
      "Feature 1347: 0.000000\n",
      "Feature 1348: 0.000000\n",
      "Feature 1349: 0.046115\n",
      "Feature 1350: 0.086449\n",
      "Feature 1351: 0.017809\n",
      "Feature 1352: 0.000000\n",
      "Feature 1353: 0.000000\n",
      "Feature 1354: 0.117861\n",
      "Feature 1355: 0.000000\n",
      "Feature 1356: 0.000000\n",
      "Feature 1357: 0.000000\n",
      "Feature 1358: 0.000000\n",
      "Feature 1359: 0.041155\n",
      "Feature 1360: 0.006979\n",
      "Feature 1361: 0.000000\n",
      "Feature 1362: 0.000000\n",
      "Feature 1363: 0.062078\n",
      "Feature 1364: 0.047265\n",
      "Feature 1365: 0.000000\n",
      "Feature 1366: 0.009616\n",
      "Feature 1367: 0.014540\n",
      "Feature 1368: 0.046796\n",
      "Feature 1369: 0.000000\n",
      "Feature 1370: 0.000000\n",
      "Feature 1371: 0.003594\n",
      "Feature 1372: 0.000000\n",
      "Feature 1373: 0.000000\n",
      "Feature 1374: 0.000000\n",
      "Feature 1375: 0.061016\n",
      "Feature 1376: 0.003537\n",
      "Feature 1377: 0.000000\n",
      "Feature 1378: 0.159669\n",
      "Feature 1379: 0.000000\n",
      "Feature 1380: 0.048452\n",
      "Feature 1381: 0.014388\n",
      "Feature 1382: 0.000118\n",
      "Feature 1383: 0.000000\n",
      "Feature 1384: 0.012837\n",
      "Feature 1385: 0.000000\n",
      "Feature 1386: 0.067137\n",
      "Feature 1387: 0.006554\n",
      "Feature 1388: 0.000000\n",
      "Feature 1389: 0.000000\n",
      "Feature 1390: 0.070439\n",
      "Feature 1391: 0.029649\n",
      "Feature 1392: 0.022832\n",
      "Feature 1393: 0.168592\n",
      "Feature 1394: 0.021701\n",
      "Feature 1395: 0.000000\n",
      "Feature 1396: 0.048428\n",
      "Feature 1397: 0.022603\n",
      "Feature 1398: 0.000000\n",
      "Feature 1399: 0.000000\n",
      "Feature 1400: 0.080589\n",
      "Feature 1401: 0.104073\n",
      "Feature 1402: 0.000000\n",
      "Feature 1403: 0.000000\n",
      "Feature 1404: 0.033282\n",
      "Feature 1405: 0.056895\n",
      "Feature 1406: 0.000000\n",
      "Feature 1407: 0.029575\n",
      "Feature 1408: 0.019913\n",
      "Feature 1409: 0.070344\n",
      "Feature 1410: 0.051529\n",
      "Feature 1411: 0.038595\n",
      "Feature 1412: 0.000000\n",
      "Feature 1413: 0.022568\n",
      "Feature 1414: 0.059644\n",
      "Feature 1415: 0.000000\n",
      "Feature 1416: 0.070723\n",
      "Feature 1417: 0.039204\n",
      "Feature 1418: 0.000000\n",
      "Feature 1419: 0.061950\n",
      "Feature 1420: 0.000000\n",
      "Feature 1421: 0.042572\n",
      "Feature 1422: 0.029738\n",
      "Feature 1423: 0.000000\n",
      "Feature 1424: 0.100833\n",
      "Feature 1425: 0.038609\n",
      "Feature 1426: 0.015111\n",
      "Feature 1427: 0.000000\n",
      "Feature 1428: 0.000000\n",
      "Feature 1429: 0.000000\n",
      "Feature 1430: 0.051845\n",
      "Feature 1431: 0.025824\n",
      "Feature 1432: 0.093209\n",
      "Feature 1433: 0.108134\n",
      "Feature 1434: 0.000000\n",
      "Feature 1435: 0.000000\n",
      "Feature 1436: 0.000000\n",
      "Feature 1437: 0.000000\n",
      "Feature 1438: 0.000000\n",
      "Feature 1439: 0.105391\n",
      "Feature 1440: 0.066346\n",
      "Feature 1441: 0.000000\n",
      "Feature 1442: 0.023150\n",
      "Feature 1443: 0.000000\n",
      "Feature 1444: 0.000000\n",
      "Feature 1445: 0.064797\n",
      "Feature 1446: 0.015824\n",
      "Feature 1447: 0.000000\n",
      "Feature 1448: 0.013981\n",
      "Feature 1449: 0.126550\n",
      "Feature 1450: 0.030168\n",
      "Feature 1451: 0.000000\n",
      "Feature 1452: 0.000000\n",
      "Feature 1453: 0.000000\n",
      "Feature 1454: 0.000000\n",
      "Feature 1455: 0.000000\n",
      "Feature 1456: 0.008585\n",
      "Feature 1457: 0.055695\n",
      "Feature 1458: 0.052516\n",
      "Feature 1459: 0.000000\n",
      "Feature 1460: 0.000000\n",
      "Feature 1461: 0.000000\n",
      "Feature 1462: 0.000000\n",
      "Feature 1463: 0.000000\n",
      "Feature 1464: 0.000000\n",
      "Feature 1465: 0.086730\n",
      "Feature 1466: 0.000000\n",
      "Feature 1467: 0.037262\n",
      "Feature 1468: 0.000000\n",
      "Feature 1469: 0.000000\n",
      "Feature 1470: 0.000000\n",
      "Feature 1471: 0.056981\n",
      "Feature 1472: 0.000000\n",
      "Feature 1473: 0.000000\n",
      "Feature 1474: 0.181524\n",
      "Feature 1475: 0.000000\n",
      "Feature 1476: 0.048280\n",
      "Feature 1477: 0.023328\n",
      "Feature 1478: 0.000000\n",
      "Feature 1479: 0.000000\n",
      "Feature 1480: 0.000000\n",
      "Feature 1481: 0.017644\n",
      "Feature 1482: 0.033769\n",
      "Feature 1483: 0.068078\n",
      "Feature 1484: 0.149022\n",
      "Feature 1485: 0.055927\n",
      "Feature 1486: 0.000000\n",
      "Feature 1487: 0.046427\n",
      "Feature 1488: 0.071404\n",
      "Feature 1489: 0.095321\n",
      "Feature 1490: 0.000000\n",
      "Feature 1491: 0.000000\n",
      "Feature 1492: 0.060678\n",
      "Feature 1493: 0.024365\n",
      "Feature 1494: 0.000000\n",
      "Feature 1495: 0.052599\n",
      "Feature 1496: 0.000000\n",
      "Feature 1497: 0.106046\n",
      "Feature 1498: 0.000000\n",
      "Feature 1499: 0.000000\n",
      "Feature 1500: 0.084617\n",
      "Feature 1501: 0.041672\n",
      "Feature 1502: 0.135993\n",
      "Feature 1503: 0.051668\n",
      "Feature 1504: 0.000000\n",
      "Feature 1505: 0.000000\n",
      "Feature 1506: 0.000000\n",
      "Feature 1507: 0.134206\n",
      "Feature 1508: 0.011841\n",
      "Feature 1509: 0.000000\n",
      "Feature 1510: 0.044079\n",
      "Feature 1511: 0.000000\n",
      "Feature 1512: 0.000000\n",
      "Feature 1513: 0.000000\n",
      "Feature 1514: 0.000000\n",
      "Feature 1515: 0.008369\n",
      "Feature 1516: 0.000000\n",
      "Feature 1517: 0.000000\n",
      "Feature 1518: 0.000000\n",
      "Feature 1519: 0.022848\n",
      "Feature 1520: 0.000000\n",
      "Feature 1521: 0.000000\n",
      "Feature 1522: 0.000000\n",
      "Feature 1523: 0.000000\n",
      "Feature 1524: 0.017736\n",
      "Feature 1525: 0.000000\n",
      "Feature 1526: 0.077270\n",
      "Feature 1527: 0.045299\n",
      "Feature 1528: 0.000000\n",
      "Feature 1529: 0.000000\n",
      "Feature 1530: 0.000000\n",
      "Feature 1531: 0.000000\n",
      "Feature 1532: 0.079850\n",
      "Feature 1533: 0.000000\n",
      "Feature 1534: 0.040308\n",
      "Feature 1535: 0.000000\n",
      "Feature 1536: 0.000000\n",
      "Feature 1537: 0.000000\n",
      "Feature 1538: 0.000000\n",
      "Feature 1539: 0.000000\n",
      "Feature 1540: 0.000000\n",
      "Feature 1541: 0.000000\n",
      "Feature 1542: 0.000000\n",
      "Feature 1543: 0.000000\n",
      "Feature 1544: 0.000000\n",
      "Feature 1545: 0.000000\n",
      "Feature 1546: 0.001670\n",
      "Feature 1547: 0.000000\n",
      "Feature 1548: 0.000000\n",
      "Feature 1549: 0.021910\n",
      "Feature 1550: 0.000000\n",
      "Feature 1551: 0.000000\n",
      "Feature 1552: 0.000000\n",
      "Feature 1553: 0.000000\n",
      "Feature 1554: 0.128973\n",
      "Feature 1555: 0.000000\n",
      "Feature 1556: 0.000000\n",
      "Feature 1557: 0.000000\n",
      "Feature 1558: 0.000000\n",
      "Feature 1559: 0.000000\n",
      "Feature 1560: 0.010174\n",
      "Feature 1561: 0.098999\n",
      "Feature 1562: 0.155060\n",
      "Feature 1563: 0.238726\n",
      "Feature 1564: 0.000000\n",
      "Feature 1565: 0.121565\n",
      "Feature 1566: 0.093084\n",
      "Feature 1567: 0.000993\n",
      "Feature 1568: 0.000000\n",
      "Feature 1569: 0.012760\n",
      "Feature 1570: 0.108645\n",
      "Feature 1571: 0.056116\n",
      "Feature 1572: 0.024684\n",
      "Feature 1573: 0.000000\n",
      "Feature 1574: 0.000000\n",
      "Feature 1575: 0.000000\n",
      "Feature 1576: 0.005678\n",
      "Feature 1577: 0.048469\n",
      "Feature 1578: 0.000000\n",
      "Feature 1579: 0.000000\n",
      "Feature 1580: 0.000000\n",
      "Feature 1581: 0.161000\n",
      "Feature 1582: 0.000000\n",
      "Feature 1583: 0.005763\n",
      "Feature 1584: 0.000000\n",
      "Feature 1585: 0.000000\n",
      "Feature 1586: 0.000000\n",
      "Feature 1587: 0.000000\n",
      "Feature 1588: 0.037884\n",
      "Feature 1589: 0.000000\n",
      "Feature 1590: 0.000000\n",
      "Feature 1591: 0.037618\n",
      "Feature 1592: 0.182934\n",
      "Feature 1593: 0.028612\n",
      "Feature 1594: 0.056370\n",
      "Feature 1595: 0.000000\n",
      "Feature 1596: 0.000000\n",
      "Feature 1597: 0.010420\n",
      "Feature 1598: 0.171414\n",
      "Feature 1599: 0.033247\n",
      "Feature 1600: 0.053598\n",
      "Feature 1601: 0.014188\n",
      "Feature 1602: 0.003815\n",
      "Feature 1603: 0.007329\n",
      "Feature 1604: 0.075827\n",
      "Feature 1605: 0.000000\n",
      "Feature 1606: 0.036667\n",
      "Feature 1607: 0.000000\n",
      "Feature 1608: 0.000000\n",
      "Feature 1609: 0.073296\n",
      "Feature 1610: 0.060933\n",
      "Feature 1611: 0.000000\n",
      "Feature 1612: 0.012946\n",
      "Feature 1613: 0.000000\n",
      "Feature 1614: 0.047665\n",
      "Feature 1615: 0.005951\n",
      "Feature 1616: 0.000000\n",
      "Feature 1617: 0.000000\n",
      "Feature 1618: 0.152139\n",
      "Feature 1619: 0.000000\n",
      "Feature 1620: 0.000000\n",
      "Feature 1621: 0.011789\n",
      "Feature 1622: 0.000000\n",
      "Feature 1623: 0.009456\n",
      "Feature 1624: 0.000000\n",
      "Feature 1625: 0.036421\n",
      "Feature 1626: 0.000000\n",
      "Feature 1627: 0.088243\n",
      "Feature 1628: 0.000000\n",
      "Feature 1629: 0.000000\n",
      "Feature 1630: 0.000000\n",
      "Feature 1631: 0.000000\n",
      "Feature 1632: 0.088895\n",
      "Feature 1633: 0.026818\n",
      "Feature 1634: 0.000000\n",
      "Feature 1635: 0.117257\n",
      "Feature 1636: 0.000000\n",
      "Feature 1637: 0.016489\n",
      "Feature 1638: 0.108864\n",
      "Feature 1639: 0.026477\n",
      "Feature 1640: 0.082710\n",
      "Feature 1641: 0.098379\n",
      "Feature 1642: 0.136596\n",
      "Feature 1643: 0.068495\n",
      "Feature 1644: 0.041908\n",
      "Feature 1645: 0.122742\n",
      "Feature 1646: 0.181655\n",
      "Feature 1647: 0.026962\n",
      "Feature 1648: 0.136359\n",
      "Feature 1649: 0.094271\n",
      "Feature 1650: 0.072074\n",
      "Feature 1651: 0.072372\n",
      "Feature 1652: 0.000000\n",
      "Feature 1653: 0.072244\n",
      "Feature 1654: 0.077450\n",
      "Feature 1655: 0.152392\n",
      "Feature 1656: 0.259170\n",
      "Feature 1657: 0.109466\n",
      "Feature 1658: 0.087891\n",
      "Feature 1659: 0.053145\n",
      "Feature 1660: 0.087092\n",
      "Feature 1661: 0.027533\n",
      "Feature 1662: 0.138245\n",
      "Feature 1663: 0.168086\n",
      "Feature 1664: 0.140280\n",
      "Feature 1665: 0.096413\n",
      "Feature 1666: 0.131275\n",
      "Feature 1667: 0.100100\n",
      "Feature 1668: 0.106903\n",
      "Feature 1669: 0.043202\n",
      "Feature 1670: 0.067573\n",
      "Feature 1671: 0.121135\n",
      "Feature 1672: 0.075811\n",
      "Feature 1673: 0.191293\n",
      "Feature 1674: 0.049124\n",
      "Feature 1675: 0.131736\n",
      "Feature 1676: 0.127886\n",
      "Feature 1677: 0.129978\n",
      "Feature 1678: 0.168360\n",
      "Feature 1679: 0.090812\n",
      "Feature 1680: 0.078741\n",
      "Feature 1681: 0.147727\n",
      "Feature 1682: 0.000000\n",
      "Feature 1683: 0.020194\n",
      "Feature 1684: 0.115852\n",
      "Feature 1685: 0.035093\n",
      "Feature 1686: 0.097761\n",
      "Feature 1687: 0.109064\n",
      "Feature 1688: 0.106563\n",
      "Feature 1689: 0.223392\n",
      "Feature 1690: 0.089423\n",
      "Feature 1691: 0.072157\n",
      "Feature 1692: 0.000000\n",
      "Feature 1693: 0.072407\n",
      "Feature 1694: 0.031105\n",
      "Feature 1695: 0.144200\n",
      "Feature 1696: 0.004736\n",
      "Feature 1697: 0.000000\n",
      "Feature 1698: 0.000000\n",
      "Feature 1699: 0.017895\n",
      "Feature 1700: 0.045369\n",
      "Feature 1701: 0.080785\n",
      "Feature 1702: 0.000000\n",
      "Feature 1703: 0.023222\n",
      "Feature 1704: 0.008343\n",
      "Feature 1705: 0.000000\n",
      "Feature 1706: 0.000000\n",
      "Feature 1707: 0.000000\n",
      "Feature 1708: 0.115921\n",
      "Feature 1709: 0.021848\n",
      "Feature 1710: 0.000000\n",
      "Feature 1711: 0.000000\n",
      "Feature 1712: 0.012678\n",
      "Feature 1713: 0.038650\n",
      "Feature 1714: 0.000000\n",
      "Feature 1715: 0.023945\n",
      "Feature 1716: 0.000000\n",
      "Feature 1717: 0.012534\n",
      "Feature 1718: 0.009147\n",
      "Feature 1719: 0.058713\n",
      "Feature 1720: 0.000000\n",
      "Feature 1721: 0.100841\n",
      "Feature 1722: 0.071893\n",
      "Feature 1723: 0.000000\n",
      "Feature 1724: 0.038033\n",
      "Feature 1725: 0.000000\n",
      "Feature 1726: 0.194485\n",
      "Feature 1727: 0.000000\n",
      "Feature 1728: 0.000000\n",
      "Feature 1729: 0.000000\n",
      "Feature 1730: 0.024558\n",
      "Feature 1731: 0.000000\n",
      "Feature 1732: 0.000000\n",
      "Feature 1733: 0.063620\n",
      "Feature 1734: 0.000000\n",
      "Feature 1735: 0.000000\n",
      "Feature 1736: 0.170442\n",
      "Feature 1737: 0.038734\n",
      "Feature 1738: 0.035968\n",
      "Feature 1739: 0.067275\n",
      "Feature 1740: 0.022564\n",
      "Feature 1741: 0.067098\n",
      "Feature 1742: 0.038390\n",
      "Feature 1743: 0.080802\n",
      "Feature 1744: 0.082626\n",
      "Feature 1745: 0.033595\n",
      "Feature 1746: 0.012742\n",
      "Feature 1747: 0.000000\n",
      "Feature 1748: 0.049027\n",
      "Feature 1749: 0.000000\n",
      "Feature 1750: 0.000000\n",
      "Feature 1751: 0.018607\n",
      "Feature 1752: 0.000000\n",
      "Feature 1753: 0.081189\n",
      "Feature 1754: 0.040550\n",
      "Feature 1755: 0.000000\n",
      "Feature 1756: 0.003105\n",
      "Feature 1757: 0.018332\n",
      "Feature 1758: 0.009437\n",
      "Feature 1759: 0.000486\n",
      "Feature 1760: 0.059866\n",
      "Feature 1761: 0.022000\n",
      "Feature 1762: 0.000000\n",
      "Feature 1763: 0.120833\n",
      "Feature 1764: 0.000000\n",
      "Feature 1765: 0.000000\n",
      "Feature 1766: 0.000000\n",
      "Feature 1767: 0.078607\n",
      "Feature 1768: 0.070005\n",
      "Feature 1769: 0.000000\n",
      "Feature 1770: 0.012513\n",
      "Feature 1771: 0.000000\n",
      "Feature 1772: 0.000000\n",
      "Feature 1773: 0.000000\n",
      "Feature 1774: 0.017905\n",
      "Feature 1775: 0.000000\n",
      "Feature 1776: 0.000000\n",
      "Feature 1777: 0.017523\n",
      "Feature 1778: 0.000000\n",
      "Feature 1779: 0.072335\n",
      "Feature 1780: 0.000000\n",
      "Feature 1781: 0.018235\n",
      "Feature 1782: 0.000000\n",
      "Feature 1783: 0.000000\n",
      "Feature 1784: 0.015881\n",
      "Feature 1785: 0.000000\n",
      "Feature 1786: 0.000000\n",
      "Feature 1787: 0.000000\n",
      "Feature 1788: 0.109055\n",
      "Feature 1789: 0.004206\n",
      "Feature 1790: 0.000000\n",
      "Feature 1791: 0.000000\n",
      "Feature 1792: 0.000000\n",
      "Feature 1793: 0.028712\n",
      "Feature 1794: 0.000000\n",
      "Feature 1795: 0.042573\n",
      "Feature 1796: 0.000000\n",
      "Feature 1797: 0.000000\n",
      "Feature 1798: 0.000000\n",
      "Feature 1799: 0.000000\n",
      "Feature 1800: 0.121947\n",
      "Feature 1801: 0.019116\n",
      "Feature 1802: 0.000000\n",
      "Feature 1803: 0.000000\n",
      "Feature 1804: 0.074941\n",
      "Feature 1805: 0.000000\n",
      "Feature 1806: 0.000000\n",
      "Feature 1807: 0.043835\n",
      "Feature 1808: 0.000000\n",
      "Feature 1809: 0.000000\n",
      "Feature 1810: 0.000000\n",
      "Feature 1811: 0.000000\n",
      "Feature 1812: 0.000000\n",
      "Feature 1813: 0.015004\n",
      "Feature 1814: 0.049586\n",
      "Feature 1815: 0.028527\n",
      "Feature 1816: 0.000000\n",
      "Feature 1817: 0.000000\n",
      "Feature 1818: 0.000000\n",
      "Feature 1819: 0.000000\n",
      "Feature 1820: 0.000000\n",
      "Feature 1821: 0.000000\n",
      "Feature 1822: 0.060320\n",
      "Feature 1823: 0.000000\n",
      "Feature 1824: 0.000000\n",
      "Feature 1825: 0.000000\n",
      "Feature 1826: 0.000000\n",
      "Feature 1827: 0.037620\n",
      "Feature 1828: 0.000000\n",
      "Feature 1829: 0.000072\n",
      "Feature 1830: 0.034921\n",
      "Feature 1831: 0.177423\n",
      "Feature 1832: 0.000000\n",
      "Feature 1833: 0.015055\n",
      "Feature 1834: 0.000000\n",
      "Feature 1835: 0.000000\n",
      "Feature 1836: 0.056782\n",
      "Feature 1837: 0.032957\n",
      "Feature 1838: 0.124662\n",
      "Feature 1839: 0.043779\n",
      "Feature 1840: 0.000000\n",
      "Feature 1841: 0.000000\n",
      "Feature 1842: 0.029997\n",
      "Feature 1843: 0.062455\n",
      "Feature 1844: 0.000000\n",
      "Feature 1845: 0.086813\n",
      "Feature 1846: 0.016069\n",
      "Feature 1847: 0.000000\n",
      "Feature 1848: 0.087134\n",
      "Feature 1849: 0.000000\n",
      "Feature 1850: 0.000000\n",
      "Feature 1851: 0.059459\n",
      "Feature 1852: 0.000000\n",
      "Feature 1853: 0.000000\n",
      "Feature 1854: 0.072059\n",
      "Feature 1855: 0.010479\n",
      "Feature 1856: 0.003936\n",
      "Feature 1857: 0.082405\n",
      "Feature 1858: 0.042984\n",
      "Feature 1859: 0.057153\n",
      "Feature 1860: 0.062803\n",
      "Feature 1861: 0.153668\n",
      "Feature 1862: 0.046119\n",
      "Feature 1863: 0.000000\n",
      "Feature 1864: 0.000000\n",
      "Feature 1865: 0.037027\n",
      "Feature 1866: 0.101739\n",
      "Feature 1867: 0.019742\n",
      "Feature 1868: 0.116531\n",
      "Feature 1869: 0.002493\n",
      "Feature 1870: 0.173394\n",
      "Feature 1871: 0.098571\n",
      "Feature 1872: 0.000000\n",
      "Feature 1873: 0.087985\n",
      "Feature 1874: 0.019287\n",
      "Feature 1875: 0.000000\n",
      "Feature 1876: 0.053024\n",
      "Feature 1877: 0.000000\n",
      "Feature 1878: 0.003311\n",
      "Feature 1879: 0.028470\n",
      "Feature 1880: 0.000000\n",
      "Feature 1881: 0.007029\n",
      "Feature 1882: 0.053215\n",
      "Feature 1883: 0.000000\n",
      "Feature 1884: 0.016148\n",
      "Feature 1885: 0.135935\n",
      "Feature 1886: 0.000000\n",
      "Feature 1887: 0.009754\n",
      "Feature 1888: 0.000000\n",
      "Feature 1889: 0.000000\n",
      "Feature 1890: 0.000000\n",
      "Feature 1891: 0.076838\n",
      "Feature 1892: 0.000000\n",
      "Feature 1893: 0.000000\n",
      "Feature 1894: 0.000000\n",
      "Feature 1895: 0.000000\n",
      "Feature 1896: 0.088565\n",
      "Feature 1897: 0.009369\n",
      "Feature 1898: 0.000000\n",
      "Feature 1899: 0.026435\n",
      "Feature 1900: 0.150252\n",
      "Feature 1901: 0.013491\n",
      "Feature 1902: 0.000000\n",
      "Feature 1903: 0.002423\n",
      "Feature 1904: 0.099918\n",
      "Feature 1905: 0.017625\n",
      "Feature 1906: 0.171263\n",
      "Feature 1907: 0.044808\n",
      "Feature 1908: 0.045938\n",
      "Feature 1909: 0.002974\n",
      "Feature 1910: 0.107341\n",
      "Feature 1911: 0.000000\n",
      "Feature 1912: 0.208057\n",
      "Feature 1913: 0.129105\n",
      "Feature 1914: 0.000000\n",
      "Feature 1915: 0.000000\n",
      "Feature 1916: 0.057420\n",
      "Feature 1917: 0.052492\n",
      "Feature 1918: 0.000000\n",
      "Feature 1919: 0.048130\n",
      "Feature 1920: 0.027793\n",
      "Feature 1921: 0.000000\n",
      "Feature 1922: 0.015426\n",
      "Feature 1923: 0.115098\n",
      "Feature 1924: 0.035107\n",
      "Feature 1925: 0.087406\n",
      "Feature 1926: 0.000000\n",
      "Feature 1927: 0.000000\n",
      "Feature 1928: 0.000000\n",
      "Feature 1929: 0.000000\n",
      "Feature 1930: 0.019704\n",
      "Feature 1931: 0.000000\n",
      "Feature 1932: 0.000000\n",
      "Feature 1933: 0.000000\n",
      "Feature 1934: 0.000000\n",
      "Feature 1935: 0.000000\n",
      "Feature 1936: 0.000000\n",
      "Feature 1937: 0.000000\n",
      "Feature 1938: 0.000000\n",
      "Feature 1939: 0.000000\n",
      "Feature 1940: 0.000000\n",
      "Feature 1941: 0.000000\n",
      "Feature 1942: 0.000000\n",
      "Feature 1943: 0.000000\n",
      "Feature 1944: 0.000000\n",
      "Feature 1945: 0.019664\n",
      "Feature 1946: 0.063317\n",
      "Feature 1947: 0.000000\n",
      "Feature 1948: 0.000000\n",
      "Feature 1949: 0.000000\n",
      "Feature 1950: 0.036954\n",
      "Feature 1951: 0.000000\n",
      "Feature 1952: 0.000000\n",
      "Feature 1953: 0.000000\n",
      "Feature 1954: 0.033281\n",
      "Feature 1955: 0.000000\n",
      "Feature 1956: 0.049168\n",
      "Feature 1957: 0.000000\n",
      "Feature 1958: 0.025111\n",
      "Feature 1959: 0.000000\n",
      "Feature 1960: 0.018436\n",
      "Feature 1961: 0.008446\n",
      "Feature 1962: 0.037112\n",
      "Feature 1963: 0.000000\n",
      "Feature 1964: 0.000000\n",
      "Feature 1965: 0.000000\n",
      "Feature 1966: 0.071141\n",
      "Feature 1967: 0.022906\n",
      "Feature 1968: 0.008455\n",
      "Feature 1969: 0.000000\n",
      "Feature 1970: 0.004955\n",
      "Feature 1971: 0.160385\n",
      "Feature 1972: 0.000000\n",
      "Feature 1973: 0.047035\n",
      "Feature 1974: 0.000000\n",
      "Feature 1975: 0.000000\n",
      "Feature 1976: 0.003848\n",
      "Feature 1977: 0.000000\n",
      "Feature 1978: 0.046508\n",
      "Feature 1979: 0.073269\n",
      "Feature 1980: 0.036257\n",
      "Feature 1981: 0.042396\n",
      "Feature 1982: 0.000000\n",
      "Feature 1983: 0.020631\n",
      "Feature 1984: 0.000000\n",
      "Feature 1985: 0.001710\n",
      "Feature 1986: 0.038814\n",
      "Feature 1987: 0.026383\n",
      "Feature 1988: 0.011173\n",
      "Feature 1989: 0.000000\n",
      "Feature 1990: 0.073274\n",
      "Feature 1991: 0.000000\n",
      "Feature 1992: 0.030500\n",
      "Feature 1993: 0.142700\n",
      "Feature 1994: 0.031160\n",
      "Feature 1995: 0.000000\n",
      "Feature 1996: 0.041027\n",
      "Feature 1997: 0.000000\n",
      "Feature 1998: 0.000000\n",
      "Feature 1999: 0.000000\n",
      "Feature 2000: 0.018496\n",
      "Feature 2001: 0.098780\n",
      "Feature 2002: 0.051265\n",
      "Feature 2003: 0.000000\n",
      "Feature 2004: 0.009089\n",
      "Feature 2005: 0.000000\n",
      "Feature 2006: 0.000000\n",
      "Feature 2007: 0.100355\n",
      "Feature 2008: 0.097229\n",
      "Feature 2009: 0.000000\n",
      "Feature 2010: 0.019093\n",
      "Feature 2011: 0.000000\n",
      "Feature 2012: 0.000000\n",
      "Feature 2013: 0.051055\n",
      "Feature 2014: 0.000000\n",
      "Feature 2015: 0.000000\n",
      "Feature 2016: 0.088462\n",
      "Feature 2017: 0.032758\n",
      "Feature 2018: 0.095858\n",
      "Feature 2019: 0.082565\n",
      "Feature 2020: 0.038796\n",
      "Feature 2021: 0.000000\n",
      "Feature 2022: 0.151700\n",
      "Feature 2023: 0.133475\n",
      "Feature 2024: 0.137881\n",
      "Feature 2025: 0.058858\n",
      "Feature 2026: 0.098964\n",
      "Feature 2027: 0.034356\n",
      "Feature 2028: 0.000000\n",
      "Feature 2029: 0.000000\n",
      "Feature 2030: 0.005889\n",
      "Feature 2031: 0.013620\n",
      "Feature 2032: 0.000000\n",
      "Feature 2033: 0.062822\n",
      "Feature 2034: 0.078598\n",
      "Feature 2035: 0.057673\n",
      "Feature 2036: 0.063313\n",
      "Feature 2037: 0.000000\n",
      "Feature 2038: 0.000000\n",
      "Feature 2039: 0.059773\n",
      "Feature 2040: 0.033098\n",
      "Feature 2041: 0.079142\n",
      "Feature 2042: 0.000000\n",
      "Feature 2043: 0.044372\n",
      "Feature 2044: 0.000000\n",
      "Feature 2045: 0.011226\n",
      "Feature 2046: 0.034199\n",
      "Feature 2047: 0.000000\n",
      "Feature 2048: 0.000000\n",
      "Feature 2049: 0.084659\n",
      "Feature 2050: 0.049754\n",
      "Feature 2051: 0.000000\n",
      "Feature 2052: 0.000000\n",
      "Feature 2053: 0.060378\n",
      "Feature 2054: 0.123623\n",
      "Feature 2055: 0.034078\n",
      "Feature 2056: 0.024228\n",
      "Feature 2057: 0.000000\n",
      "Feature 2058: 0.000000\n",
      "Feature 2059: 0.061759\n",
      "Feature 2060: 0.065484\n",
      "Feature 2061: 0.057559\n",
      "Feature 2062: 0.037713\n",
      "Feature 2063: 0.005524\n",
      "Feature 2064: 0.162376\n",
      "Feature 2065: 0.205585\n",
      "Feature 2066: 0.032289\n",
      "Feature 2067: 0.143060\n",
      "Feature 2068: 0.009826\n",
      "Feature 2069: 0.028140\n",
      "Feature 2070: 0.143461\n",
      "Feature 2071: 0.146562\n",
      "Feature 2072: 0.066771\n",
      "Feature 2073: 0.000000\n",
      "Feature 2074: 0.000000\n",
      "Feature 2075: 0.075247\n",
      "Feature 2076: 0.000000\n",
      "Feature 2077: 0.063752\n",
      "Feature 2078: 0.020976\n",
      "Feature 2079: 0.031084\n",
      "Feature 2080: 0.000000\n",
      "Feature 2081: 0.004069\n",
      "Feature 2082: 0.177845\n",
      "Feature 2083: 0.000000\n",
      "Feature 2084: 0.000000\n",
      "Feature 2085: 0.000000\n",
      "Feature 2086: 0.036706\n",
      "Feature 2087: 0.000000\n",
      "Feature 2088: 0.000000\n",
      "Feature 2089: 0.000000\n",
      "Feature 2090: 0.000000\n",
      "Feature 2091: 0.000000\n",
      "Feature 2092: 0.000000\n",
      "Feature 2093: 0.033690\n",
      "Feature 2094: 0.054958\n",
      "Feature 2095: 0.000000\n",
      "Feature 2096: 0.021513\n",
      "Feature 2097: 0.015057\n",
      "Feature 2098: 0.000000\n",
      "Feature 2099: 0.000000\n",
      "Feature 2100: 0.000000\n",
      "Feature 2101: 0.019208\n",
      "Feature 2102: 0.000000\n",
      "Feature 2103: 0.000000\n",
      "Feature 2104: 0.073926\n",
      "Feature 2105: 0.120046\n",
      "Feature 2106: 0.123348\n",
      "Feature 2107: 0.000941\n",
      "Feature 2108: 0.132822\n",
      "Feature 2109: 0.000000\n",
      "Feature 2110: 0.009820\n",
      "Feature 2111: 0.015584\n",
      "Feature 2112: 0.095281\n",
      "Feature 2113: 0.121965\n",
      "Feature 2114: 0.002763\n",
      "Feature 2115: 0.012803\n",
      "Feature 2116: 0.126270\n",
      "Feature 2117: 0.000000\n",
      "Feature 2118: 0.000000\n",
      "Feature 2119: 0.079293\n",
      "Feature 2120: 0.000000\n",
      "Feature 2121: 0.086280\n",
      "Feature 2122: 0.115743\n",
      "Feature 2123: 0.040705\n",
      "Feature 2124: 0.167784\n",
      "Feature 2125: 0.000000\n",
      "Feature 2126: 0.049438\n",
      "Feature 2127: 0.013805\n",
      "Feature 2128: 0.020413\n",
      "Feature 2129: 0.026487\n",
      "Feature 2130: 0.000000\n",
      "Feature 2131: 0.000000\n",
      "Feature 2132: 0.000000\n",
      "Feature 2133: 0.000000\n",
      "Feature 2134: 0.093741\n",
      "Feature 2135: 0.028172\n",
      "Feature 2136: 0.000000\n",
      "Feature 2137: 0.015365\n",
      "Feature 2138: 0.000822\n",
      "Feature 2139: 0.000000\n",
      "Feature 2140: 0.023037\n",
      "Feature 2141: 0.000000\n",
      "Feature 2142: 0.000000\n",
      "Feature 2143: 0.022508\n",
      "Feature 2144: 0.007686\n",
      "Feature 2145: 0.127416\n",
      "Feature 2146: 0.000000\n",
      "Feature 2147: 0.000000\n",
      "Feature 2148: 0.000000\n",
      "Feature 2149: 0.000000\n",
      "Feature 2150: 0.000000\n",
      "Feature 2151: 0.000000\n",
      "Feature 2152: 0.000000\n",
      "Feature 2153: 0.069427\n",
      "Feature 2154: 0.097518\n",
      "Feature 2155: 0.228168\n",
      "Feature 2156: 0.000000\n",
      "Feature 2157: 0.090976\n",
      "Feature 2158: 0.076197\n",
      "Feature 2159: 0.110710\n",
      "Feature 2160: 0.052630\n",
      "Feature 2161: 0.001672\n",
      "Feature 2162: 0.050141\n",
      "Feature 2163: 0.099878\n",
      "Feature 2164: 0.043438\n",
      "Feature 2165: 0.038149\n",
      "Feature 2166: 0.082995\n",
      "Feature 2167: 0.026326\n",
      "Feature 2168: 0.043083\n",
      "Feature 2169: 0.014178\n",
      "Feature 2170: 0.020610\n",
      "Feature 2171: 0.000000\n",
      "Feature 2172: 0.207579\n",
      "Feature 2173: 0.000000\n",
      "Feature 2174: 0.000000\n",
      "Feature 2175: 0.000000\n",
      "Feature 2176: 0.031499\n",
      "Feature 2177: 0.000000\n",
      "Feature 2178: 0.038609\n",
      "Feature 2179: 0.007049\n",
      "Feature 2180: 0.000000\n",
      "Feature 2181: 0.121042\n",
      "Feature 2182: 0.078695\n",
      "Feature 2183: 0.164784\n",
      "Feature 2184: 0.627577\n",
      "Feature 2185: 0.000000\n",
      "Feature 2186: 0.000000\n",
      "Feature 2187: 0.123786\n",
      "Feature 2188: 0.097774\n",
      "Feature 2189: 0.164914\n",
      "Feature 2190: 0.079553\n",
      "Feature 2191: 0.018193\n",
      "Feature 2192: 0.078031\n",
      "Feature 2193: 0.022554\n",
      "Feature 2194: 0.001373\n",
      "Feature 2195: 0.032169\n",
      "Feature 2196: 0.000000\n",
      "Feature 2197: 0.028972\n",
      "Feature 2198: 0.000000\n",
      "Feature 2199: 0.028977\n",
      "Feature 2200: 0.062952\n",
      "Feature 2201: 0.037534\n",
      "Feature 2202: 0.098104\n",
      "Feature 2203: 0.112378\n",
      "Feature 2204: 0.052866\n",
      "Feature 2205: 0.018186\n",
      "Feature 2206: 0.063512\n",
      "Feature 2207: 0.153909\n",
      "Feature 2208: 0.074998\n",
      "Feature 2209: 0.038774\n",
      "Feature 2210: 0.103879\n",
      "Feature 2211: 0.000000\n",
      "Feature 2212: 0.205315\n",
      "Feature 2213: 0.187676\n",
      "Feature 2214: 0.115265\n",
      "Feature 2215: 0.040463\n",
      "Feature 2216: 0.000000\n",
      "Feature 2217: 0.000000\n",
      "Feature 2218: 0.048431\n",
      "Feature 2219: 0.146523\n",
      "Feature 2220: 0.094157\n",
      "Feature 2221: 0.146250\n",
      "Feature 2222: 0.000000\n",
      "Feature 2223: 0.080344\n",
      "Feature 2224: 0.042939\n",
      "Feature 2225: 0.029812\n",
      "Feature 2226: 0.046065\n",
      "Feature 2227: 0.035907\n",
      "Feature 2228: 0.098721\n",
      "Feature 2229: 0.022684\n",
      "Feature 2230: 0.000000\n",
      "Feature 2231: 0.000000\n",
      "Feature 2232: 0.000000\n",
      "Feature 2233: 0.072012\n",
      "Feature 2234: 0.084663\n",
      "Feature 2235: 0.044187\n",
      "Feature 2236: 0.029425\n",
      "Feature 2237: 0.101058\n",
      "Feature 2238: 0.000000\n",
      "Feature 2239: 0.055706\n",
      "Feature 2240: 0.100694\n",
      "Feature 2241: 0.083679\n",
      "Feature 2242: 0.060679\n",
      "Feature 2243: 0.150334\n",
      "Feature 2244: 0.000000\n",
      "Feature 2245: 0.000000\n",
      "Feature 2246: 0.000000\n",
      "Feature 2247: 0.000000\n",
      "Feature 2248: 0.000000\n",
      "Feature 2249: 0.000000\n",
      "Feature 2250: 0.031674\n",
      "Feature 2251: 0.000000\n",
      "Feature 2252: 0.000000\n",
      "Feature 2253: 0.000000\n",
      "Feature 2254: 0.000000\n",
      "Feature 2255: 0.000000\n",
      "Feature 2256: 0.021053\n",
      "Feature 2257: 0.000000\n",
      "Feature 2258: 0.000000\n",
      "Feature 2259: 0.000000\n",
      "Feature 2260: 0.018828\n",
      "Feature 2261: 0.043124\n",
      "Feature 2262: 0.132768\n",
      "Feature 2263: 0.147836\n",
      "Feature 2264: 0.000000\n",
      "Feature 2265: 0.171086\n",
      "Feature 2266: 0.054346\n",
      "Feature 2267: 0.000000\n",
      "Feature 2268: 0.080554\n",
      "Feature 2269: 0.091103\n",
      "Feature 2270: 0.000000\n",
      "Feature 2271: 0.044239\n",
      "Feature 2272: 0.085126\n",
      "Feature 2273: 0.000000\n",
      "Feature 2274: 0.000000\n",
      "Feature 2275: 0.052927\n",
      "Feature 2276: 0.000000\n",
      "Feature 2277: 0.000000\n",
      "Feature 2278: 0.044022\n",
      "Feature 2279: 0.047781\n",
      "Feature 2280: 0.000000\n",
      "Feature 2281: 0.102900\n",
      "Feature 2282: 0.010022\n",
      "Feature 2283: 0.062687\n",
      "Feature 2284: 0.093488\n",
      "Feature 2285: 0.126947\n",
      "Feature 2286: 0.000000\n",
      "Feature 2287: 0.000000\n",
      "Feature 2288: 0.000000\n",
      "Feature 2289: 0.004389\n",
      "Feature 2290: 0.060629\n",
      "Feature 2291: 0.068568\n",
      "Feature 2292: 0.130993\n",
      "Feature 2293: 0.058567\n",
      "Feature 2294: 0.000000\n",
      "Feature 2295: 0.000000\n",
      "Feature 2296: 0.122207\n",
      "Feature 2297: 0.000000\n",
      "Feature 2298: 0.000000\n",
      "Feature 2299: 0.048268\n",
      "Feature 2300: 0.000000\n",
      "Feature 2301: 0.000000\n",
      "Feature 2302: 0.000000\n",
      "Feature 2303: 0.071497\n",
      "Feature 2304: 0.000000\n",
      "Feature 2305: 0.000000\n",
      "Feature 2306: 0.048351\n",
      "Feature 2307: 0.045978\n",
      "Feature 2308: 0.000216\n",
      "Feature 2309: 0.000000\n",
      "Feature 2310: 0.057280\n",
      "Feature 2311: 0.000000\n",
      "Feature 2312: 0.000000\n",
      "Feature 2313: 0.043254\n",
      "Feature 2314: 0.000000\n",
      "Feature 2315: 0.000000\n",
      "Feature 2316: 0.011005\n",
      "Feature 2317: 0.000000\n",
      "Feature 2318: 0.062686\n",
      "Feature 2319: 0.191748\n",
      "Feature 2320: 0.000000\n",
      "Feature 2321: 0.000000\n",
      "Feature 2322: 0.026052\n",
      "Feature 2323: 0.000000\n",
      "Feature 2324: 0.000000\n",
      "Feature 2325: 0.015055\n",
      "Feature 2326: 0.009100\n",
      "Feature 2327: 0.057556\n",
      "Feature 2328: 0.000000\n",
      "Feature 2329: 0.118644\n",
      "Feature 2330: 0.054028\n",
      "Feature 2331: 0.000000\n",
      "Feature 2332: 0.000000\n",
      "Feature 2333: 0.000000\n",
      "Feature 2334: 0.000000\n",
      "Feature 2335: 0.059750\n",
      "Feature 2336: 0.000000\n",
      "Feature 2337: 0.000000\n",
      "Feature 2338: 0.022131\n",
      "Feature 2339: 0.000000\n",
      "Feature 2340: 0.038589\n",
      "Feature 2341: 0.000000\n",
      "Feature 2342: 0.000000\n",
      "Feature 2343: 0.000000\n",
      "Feature 2344: 0.059897\n",
      "Feature 2345: 0.131733\n",
      "Feature 2346: 0.052678\n",
      "Feature 2347: 0.000000\n",
      "Feature 2348: 0.000000\n",
      "Feature 2349: 0.000000\n",
      "Feature 2350: 0.000000\n",
      "Feature 2351: 0.000000\n",
      "Feature 2352: 0.001266\n",
      "Feature 2353: 0.000000\n",
      "Feature 2354: 0.050009\n",
      "Feature 2355: 0.082792\n",
      "Feature 2356: 0.000000\n",
      "Feature 2357: 0.000000\n",
      "Feature 2358: 0.000000\n",
      "Feature 2359: 0.000000\n",
      "Feature 2360: 0.021628\n",
      "Feature 2361: 0.000000\n",
      "Feature 2362: 0.000000\n",
      "Feature 2363: 0.000000\n",
      "Feature 2364: 0.000000\n",
      "Feature 2365: 0.000000\n",
      "Feature 2366: 0.000000\n",
      "Feature 2367: 0.000000\n",
      "Feature 2368: 0.039782\n",
      "Feature 2369: 0.000000\n",
      "Feature 2370: 0.000000\n",
      "Feature 2371: 0.000000\n",
      "Feature 2372: 0.081755\n",
      "Feature 2373: 0.040234\n",
      "Feature 2374: 0.000000\n",
      "Feature 2375: 0.084970\n",
      "Feature 2376: 0.000000\n",
      "Feature 2377: 0.000000\n",
      "Feature 2378: 0.046953\n",
      "Feature 2379: 0.010075\n",
      "Feature 2380: 0.099920\n",
      "Feature 2381: 0.000000\n",
      "Feature 2382: 0.151030\n",
      "Feature 2383: 0.000000\n",
      "Feature 2384: 0.014278\n",
      "Feature 2385: 0.000000\n",
      "Feature 2386: 0.000000\n",
      "Feature 2387: 0.116225\n",
      "Feature 2388: 0.000000\n",
      "Feature 2389: 0.015967\n",
      "Feature 2390: 0.013091\n",
      "Feature 2391: 0.063782\n",
      "Feature 2392: 0.032084\n",
      "Feature 2393: 0.000000\n",
      "Feature 2394: 0.000000\n",
      "Feature 2395: 0.023192\n",
      "Feature 2396: 0.045138\n",
      "Feature 2397: 0.000000\n",
      "Feature 2398: 0.007039\n",
      "Feature 2399: 0.000000\n",
      "Feature 2400: 0.000000\n",
      "Feature 2401: 0.016036\n",
      "Feature 2402: 0.062687\n",
      "Feature 2403: 0.020566\n",
      "Feature 2404: 0.056874\n",
      "Feature 2405: 0.056683\n",
      "Feature 2406: 0.000000\n",
      "Feature 2407: 0.000000\n",
      "Feature 2408: 0.001106\n",
      "Feature 2409: 0.050294\n",
      "Feature 2410: 0.000000\n",
      "Feature 2411: 0.000000\n",
      "Feature 2412: 0.000000\n",
      "Feature 2413: 0.192974\n",
      "Feature 2414: 0.010907\n",
      "Feature 2415: 0.166347\n",
      "Feature 2416: 0.000000\n",
      "Feature 2417: 0.087484\n",
      "Feature 2418: 0.000000\n",
      "Feature 2419: 0.005412\n",
      "Feature 2420: 0.000000\n",
      "Feature 2421: 0.125436\n",
      "Feature 2422: 0.041371\n",
      "Feature 2423: 0.046384\n",
      "Feature 2424: 0.037631\n",
      "Feature 2425: 0.000000\n",
      "Feature 2426: 0.000000\n",
      "Feature 2427: 0.037882\n",
      "Feature 2428: 0.027841\n",
      "Feature 2429: 0.000000\n",
      "Feature 2430: 0.109801\n",
      "Feature 2431: 0.000000\n",
      "Feature 2432: 0.000000\n",
      "Feature 2433: 0.000000\n",
      "Feature 2434: 0.021968\n",
      "Feature 2435: 0.000000\n",
      "Feature 2436: 0.000000\n",
      "Feature 2437: 0.000000\n",
      "Feature 2438: 0.037156\n",
      "Feature 2439: 0.017121\n",
      "Feature 2440: 0.026069\n",
      "Feature 2441: 0.000370\n",
      "Feature 2442: 0.000000\n",
      "Feature 2443: 0.000000\n",
      "Feature 2444: 0.000000\n",
      "Feature 2445: 0.000000\n",
      "Feature 2446: 0.000000\n",
      "Feature 2447: 0.000000\n",
      "Feature 2448: 0.003344\n",
      "Feature 2449: 0.000000\n",
      "Feature 2450: 0.000000\n",
      "Feature 2451: 0.010280\n",
      "Feature 2452: 0.000000\n",
      "Feature 2453: 0.000000\n",
      "Feature 2454: 0.003200\n",
      "Feature 2455: 0.023041\n",
      "Feature 2456: 0.013281\n",
      "Feature 2457: 0.064811\n",
      "Feature 2458: 0.058052\n",
      "Feature 2459: 0.000000\n",
      "Feature 2460: 0.000000\n",
      "Feature 2461: 0.008036\n",
      "Feature 2462: 0.000000\n",
      "Feature 2463: 0.000000\n",
      "Feature 2464: 0.037880\n",
      "Feature 2465: 0.000000\n",
      "Feature 2466: 0.116456\n",
      "Feature 2467: 0.000000\n",
      "Feature 2468: 0.000000\n",
      "Feature 2469: 0.000000\n",
      "Feature 2470: 0.000000\n",
      "Feature 2471: 0.036848\n",
      "Feature 2472: 0.000000\n",
      "Feature 2473: 0.000000\n",
      "Feature 2474: 0.000000\n",
      "Feature 2475: 0.000000\n",
      "Feature 2476: 0.000000\n",
      "Feature 2477: 0.036953\n",
      "Feature 2478: 0.000000\n",
      "Feature 2479: 0.089627\n",
      "Feature 2480: 0.011048\n",
      "Feature 2481: 0.160396\n",
      "Feature 2482: 0.003242\n",
      "Feature 2483: 0.000000\n",
      "Feature 2484: 0.012154\n",
      "Feature 2485: 0.037419\n",
      "Feature 2486: 0.000000\n",
      "Feature 2487: 0.000000\n",
      "Feature 2488: 0.000000\n",
      "Feature 2489: 0.089651\n",
      "Feature 2490: 0.004465\n",
      "Feature 2491: 0.000000\n",
      "Feature 2492: 0.016173\n",
      "Feature 2493: 0.042041\n",
      "Feature 2494: 0.000000\n",
      "Feature 2495: 0.084696\n",
      "Feature 2496: 0.000000\n",
      "Feature 2497: 0.030777\n",
      "Feature 2498: 0.056504\n",
      "Feature 2499: 0.000000\n",
      "Feature 2500: 0.000000\n",
      "Feature 2501: 0.000000\n",
      "Feature 2502: 0.043343\n",
      "Feature 2503: 0.025164\n",
      "Feature 2504: 0.036520\n",
      "Feature 2505: 0.000000\n",
      "Feature 2506: 0.069385\n",
      "Feature 2507: 0.000000\n",
      "Feature 2508: 0.000000\n",
      "Feature 2509: 0.000000\n",
      "Feature 2510: 0.027160\n",
      "Feature 2511: 0.000000\n",
      "Feature 2512: 0.000000\n",
      "Feature 2513: 0.000000\n",
      "Feature 2514: 0.000000\n",
      "Feature 2515: 0.000000\n",
      "Feature 2516: 0.000000\n",
      "Feature 2517: 0.000000\n",
      "Feature 2518: 0.070216\n",
      "Feature 2519: 0.000000\n",
      "Feature 2520: 0.000000\n",
      "Feature 2521: 0.007786\n",
      "Feature 2522: 0.000000\n",
      "Feature 2523: 0.000000\n",
      "Feature 2524: 0.000000\n",
      "Feature 2525: 0.054216\n",
      "Feature 2526: 0.010028\n",
      "Feature 2527: 0.023935\n",
      "Feature 2528: 0.000000\n",
      "Feature 2529: 0.000000\n",
      "Feature 2530: 0.000000\n",
      "Feature 2531: 0.000000\n",
      "Feature 2532: 0.045788\n",
      "Feature 2533: 0.057901\n",
      "Feature 2534: 0.000000\n",
      "Feature 2535: 0.000000\n",
      "Feature 2536: 0.000000\n",
      "Feature 2537: 0.000000\n",
      "Feature 2538: 0.000000\n",
      "Feature 2539: 0.092657\n",
      "Feature 2540: 0.056057\n",
      "Feature 2541: 0.000000\n",
      "Feature 2542: 0.000000\n",
      "Feature 2543: 0.000000\n",
      "Feature 2544: 0.000000\n",
      "Feature 2545: 0.000000\n",
      "Feature 2546: 0.000000\n",
      "Feature 2547: 0.089157\n",
      "Feature 2548: 0.037996\n",
      "Feature 2549: 0.073002\n",
      "Feature 2550: 0.000000\n",
      "Feature 2551: 0.000000\n",
      "Feature 2552: 0.000000\n",
      "Feature 2553: 0.000000\n",
      "Feature 2554: 0.000000\n",
      "Feature 2555: 0.000000\n",
      "Feature 2556: 0.000000\n",
      "Feature 2557: 0.000000\n",
      "Feature 2558: 0.000000\n",
      "Feature 2559: 0.031043\n",
      "Feature 2560: 0.000000\n",
      "Feature 2561: 0.000000\n",
      "Feature 2562: 0.000000\n",
      "Feature 2563: 0.000000\n",
      "Feature 2564: 0.014302\n",
      "Feature 2565: 0.000000\n",
      "Feature 2566: 0.000000\n",
      "Feature 2567: 0.000000\n",
      "Feature 2568: 0.031734\n",
      "Feature 2569: 0.000000\n",
      "Feature 2570: 0.000000\n",
      "Feature 2571: 0.000000\n",
      "Feature 2572: 0.028523\n",
      "Feature 2573: 0.000000\n",
      "Feature 2574: 0.106116\n",
      "Feature 2575: 0.000000\n",
      "Feature 2576: 0.000000\n",
      "Feature 2577: 0.032397\n",
      "Feature 2578: 0.000000\n",
      "Feature 2579: 0.000000\n",
      "Feature 2580: 0.000000\n",
      "Feature 2581: 0.000000\n",
      "Feature 2582: 0.000000\n",
      "Feature 2583: 0.000000\n",
      "Feature 2584: 0.000000\n",
      "Feature 2585: 0.003310\n",
      "Feature 2586: 0.000000\n",
      "Feature 2587: 0.000000\n",
      "Feature 2588: 0.000000\n",
      "Feature 2589: 0.086897\n",
      "Feature 2590: 0.000000\n",
      "Feature 2591: 0.098079\n",
      "Feature 2592: 0.036769\n",
      "Feature 2593: 0.035072\n",
      "Feature 2594: 0.000000\n",
      "Feature 2595: 0.000000\n",
      "Feature 2596: 0.000000\n",
      "Feature 2597: 0.000000\n",
      "Feature 2598: 0.000000\n",
      "Feature 2599: 0.000000\n",
      "Feature 2600: 0.000000\n",
      "Feature 2601: 0.000000\n",
      "Feature 2602: 0.000000\n",
      "Feature 2603: 0.000000\n",
      "Feature 2604: 0.012779\n",
      "Feature 2605: 0.078991\n",
      "Feature 2606: 0.015054\n",
      "Feature 2607: 0.000000\n",
      "Feature 2608: 0.000000\n",
      "Feature 2609: 0.000000\n",
      "Feature 2610: 0.000000\n",
      "Feature 2611: 0.000000\n",
      "Feature 2612: 0.000000\n",
      "Feature 2613: 0.000000\n",
      "Feature 2614: 0.011353\n",
      "Feature 2615: 0.000000\n",
      "Feature 2616: 0.000000\n",
      "Feature 2617: 0.019438\n",
      "Feature 2618: 0.000000\n",
      "Feature 2619: 0.000000\n",
      "Feature 2620: 0.000000\n",
      "Feature 2621: 0.000000\n",
      "Feature 2622: 0.039316\n",
      "Feature 2623: 0.024210\n",
      "Feature 2624: 0.000000\n",
      "Feature 2625: 0.000000\n",
      "Feature 2626: 0.041862\n",
      "Feature 2627: 0.000000\n",
      "Feature 2628: 0.107773\n",
      "Feature 2629: 0.000000\n",
      "Feature 2630: 0.064279\n",
      "Feature 2631: 0.008949\n",
      "Feature 2632: 0.000000\n",
      "Feature 2633: 0.015987\n",
      "Feature 2634: 0.000000\n",
      "Feature 2635: 0.015028\n",
      "Feature 2636: 0.000000\n",
      "Feature 2637: 0.000000\n",
      "Feature 2638: 0.020712\n",
      "Feature 2639: 0.027920\n",
      "Feature 2640: 0.087337\n",
      "Feature 2641: 0.000000\n",
      "Feature 2642: 0.000000\n",
      "Feature 2643: 0.000000\n",
      "Feature 2644: 0.000000\n",
      "Feature 2645: 0.019936\n",
      "Feature 2646: 0.073553\n",
      "Feature 2647: 0.000000\n",
      "Feature 2648: 0.000000\n",
      "Feature 2649: 0.014452\n",
      "Feature 2650: 0.007265\n",
      "Feature 2651: 0.000000\n",
      "Feature 2652: 0.015019\n",
      "Feature 2653: 0.055216\n",
      "Feature 2654: 0.000000\n",
      "Feature 2655: 0.044623\n",
      "Feature 2656: 0.000000\n",
      "Feature 2657: 0.008077\n",
      "Feature 2658: 0.073839\n",
      "Feature 2659: 0.003186\n",
      "Feature 2660: 0.083021\n",
      "Feature 2661: 0.012821\n",
      "Feature 2662: 0.104444\n",
      "Feature 2663: 0.180160\n",
      "Feature 2664: 0.000000\n",
      "Feature 2665: 0.046022\n",
      "Feature 2666: 0.092279\n",
      "Feature 2667: 0.000000\n",
      "Feature 2668: 0.042046\n",
      "Feature 2669: 0.058265\n",
      "Feature 2670: 0.000000\n",
      "Feature 2671: 0.000000\n",
      "Feature 2672: 0.000000\n",
      "Feature 2673: 0.006845\n",
      "Feature 2674: 0.000000\n",
      "Feature 2675: 0.038055\n",
      "Feature 2676: 0.019934\n",
      "Feature 2677: 0.025554\n",
      "Feature 2678: 0.000000\n",
      "Feature 2679: 0.138995\n",
      "Feature 2680: 0.009230\n",
      "Feature 2681: 0.000000\n",
      "Feature 2682: 0.000000\n",
      "Feature 2683: 0.000000\n",
      "Feature 2684: 0.001406\n",
      "Feature 2685: 0.000000\n",
      "Feature 2686: 0.062380\n",
      "Feature 2687: 0.098242\n",
      "Feature 2688: 0.051381\n",
      "Feature 2689: 0.156193\n",
      "Feature 2690: 0.000000\n",
      "Feature 2691: 0.159422\n",
      "Feature 2692: 0.000000\n",
      "Feature 2693: 0.000000\n",
      "Feature 2694: 0.000000\n",
      "Feature 2695: 0.000000\n",
      "Feature 2696: 0.034007\n",
      "Feature 2697: 0.001201\n",
      "Feature 2698: 0.057175\n",
      "Feature 2699: 0.022959\n",
      "Feature 2700: 0.069630\n",
      "Feature 2701: 0.000000\n",
      "Feature 2702: 0.000000\n",
      "Feature 2703: 0.000000\n",
      "Feature 2704: 0.000000\n",
      "Feature 2705: 0.000000\n",
      "Feature 2706: 0.000000\n",
      "Feature 2707: 0.029396\n",
      "Feature 2708: 0.000000\n",
      "Feature 2709: 0.004879\n",
      "Feature 2710: 0.112640\n",
      "Feature 2711: 0.138967\n",
      "Feature 2712: 0.000000\n",
      "Feature 2713: 0.140244\n",
      "Feature 2714: 0.028807\n",
      "Feature 2715: 0.017044\n",
      "Feature 2716: 0.129907\n",
      "Feature 2717: 0.131429\n",
      "Feature 2718: 0.000000\n",
      "Feature 2719: 0.000000\n",
      "Feature 2720: 0.049994\n",
      "Feature 2721: 0.000000\n",
      "Feature 2722: 0.000000\n",
      "Feature 2723: 0.000000\n",
      "Feature 2724: 0.000000\n",
      "Feature 2725: 0.037508\n",
      "Feature 2726: 0.202868\n",
      "Feature 2727: 0.109315\n",
      "Feature 2728: 0.000000\n",
      "Feature 2729: 0.139486\n",
      "Feature 2730: 0.000000\n",
      "Feature 2731: 0.090628\n",
      "Feature 2732: 0.043690\n",
      "Feature 2733: 0.150689\n",
      "Feature 2734: 0.026353\n",
      "Feature 2735: 0.028157\n",
      "Feature 2736: 0.099519\n",
      "Feature 2737: 0.142494\n",
      "Feature 2738: 0.106442\n",
      "Feature 2739: 0.060491\n",
      "Feature 2740: 0.020049\n",
      "Feature 2741: 0.085827\n",
      "Feature 2742: 0.000000\n",
      "Feature 2743: 0.051407\n",
      "Feature 2744: 0.135901\n",
      "Feature 2745: 0.093054\n",
      "Feature 2746: 0.083253\n",
      "Feature 2747: 0.052564\n",
      "Feature 2748: 0.000000\n",
      "Feature 2749: 0.000000\n",
      "Feature 2750: 0.026083\n",
      "Feature 2751: 0.083726\n",
      "Feature 2752: 0.000000\n",
      "Feature 2753: 0.023225\n",
      "Feature 2754: 0.041400\n",
      "Feature 2755: 0.117658\n",
      "Feature 2756: 0.071640\n",
      "Feature 2757: 0.092315\n",
      "Feature 2758: 0.143570\n",
      "Feature 2759: 0.000000\n",
      "Feature 2760: 0.102984\n",
      "Feature 2761: 0.066515\n",
      "Feature 2762: 0.000000\n",
      "Feature 2763: 0.072582\n",
      "Feature 2764: 0.134011\n",
      "Feature 2765: 0.079934\n",
      "Feature 2766: 0.000000\n",
      "Feature 2767: 0.000000\n",
      "Feature 2768: 0.038174\n",
      "Feature 2769: 0.059938\n",
      "Feature 2770: 0.066820\n",
      "Feature 2771: 0.096746\n",
      "Feature 2772: 0.041759\n",
      "Feature 2773: 0.111710\n",
      "Feature 2774: 0.036244\n",
      "Feature 2775: 0.000000\n",
      "Feature 2776: 0.000000\n",
      "Feature 2777: 0.000000\n",
      "Feature 2778: 0.000000\n",
      "Feature 2779: 0.000000\n",
      "Feature 2780: 0.029716\n",
      "Feature 2781: 0.154442\n",
      "Feature 2782: 0.000000\n",
      "Feature 2783: 0.000000\n",
      "Feature 2784: 0.000000\n",
      "Feature 2785: 0.000000\n",
      "Feature 2786: 0.000000\n",
      "Feature 2787: 0.000000\n",
      "Feature 2788: 0.037076\n",
      "Feature 2789: 0.024255\n",
      "Feature 2790: 0.059487\n",
      "Feature 2791: 0.014778\n",
      "Feature 2792: 0.000000\n",
      "Feature 2793: 0.000000\n",
      "Feature 2794: 0.045078\n",
      "Feature 2795: 0.070491\n",
      "Feature 2796: 0.000000\n",
      "Feature 2797: 0.046826\n",
      "Feature 2798: 0.087613\n",
      "Feature 2799: 0.115525\n",
      "Feature 2800: 0.026549\n",
      "Feature 2801: 0.016285\n",
      "Feature 2802: 0.017276\n",
      "Feature 2803: 0.052205\n",
      "Feature 2804: 0.017391\n",
      "Feature 2805: 0.000000\n",
      "Feature 2806: 0.002719\n",
      "Feature 2807: 0.000000\n",
      "Feature 2808: 0.154799\n",
      "Feature 2809: 0.000000\n",
      "Feature 2810: 0.024553\n",
      "Feature 2811: 0.125495\n",
      "Feature 2812: 0.000000\n",
      "Feature 2813: 0.114973\n",
      "Feature 2814: 0.000000\n",
      "Feature 2815: 0.053690\n",
      "Feature 2816: 0.000947\n",
      "Feature 2817: 0.077380\n",
      "Feature 2818: 0.000000\n",
      "Feature 2819: 0.042734\n",
      "Feature 2820: 0.000000\n",
      "Feature 2821: 0.041271\n",
      "Feature 2822: 0.160178\n",
      "Feature 2823: 0.089566\n",
      "Feature 2824: 0.063281\n",
      "Feature 2825: 0.000000\n",
      "Feature 2826: 0.000000\n",
      "Feature 2827: 0.000000\n",
      "Feature 2828: 0.000000\n",
      "Feature 2829: 0.005137\n",
      "Feature 2830: 0.022582\n",
      "Feature 2831: 0.002587\n",
      "Feature 2832: 0.075648\n",
      "Feature 2833: 0.087959\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# what are scores for the features\n",
    "for i in range(len(mi_score)):\n",
    "\tprint('Feature %d: %f' % (i, mi_score[i]))\n",
    "# plot the scores\n",
    "figure(figsize=(10,8))\n",
    "pyplot.bar([i for i in range(len(mi_score))], mi_score)\n",
    "plt.title(\"Mutual information scores among all features\")\n",
    "plt.xlabel(\"All features\")\n",
    "plt.ylabel(\"Mutual information scores\")\n",
    "# plt.yticks(np.arange(0, max(mi_score), 0.02))\n",
    "# plt.axhline(y=0.18, linestyle='dashed', color='black')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean score of mutual information: 0.035446006441603464\n"
     ]
    }
   ],
   "source": [
    "# calculate the mean score\n",
    "fs_mutinfo_all_df = pd.DataFrame(mi_score)\n",
    "fs_mutinfo_all_df.rename(columns={0:'Scores'}, inplace=True)\n",
    "mean_mutinfo = fs_mutinfo_all_df['Scores'].mean()\n",
    "print('Mean score of mutual information:', mean_mutinfo)\n",
    "\n",
    "# select features higher or equal to the mean\n",
    "fs_mutinfo = fs_mutinfo_all_df.loc[fs_mutinfo_all_df['Scores'] >= 0.18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fs_mutinfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_mutinfo.sort_values(by='Scores', ascending=False).to_csv('MI_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>16</th>\n",
       "      <th>64</th>\n",
       "      <th>236</th>\n",
       "      <th>262</th>\n",
       "      <th>347</th>\n",
       "      <th>373</th>\n",
       "      <th>489</th>\n",
       "      <th>620</th>\n",
       "      <th>627</th>\n",
       "      <th>691</th>\n",
       "      <th>...</th>\n",
       "      <th>2065</th>\n",
       "      <th>2155</th>\n",
       "      <th>2172</th>\n",
       "      <th>2184</th>\n",
       "      <th>2212</th>\n",
       "      <th>2213</th>\n",
       "      <th>2319</th>\n",
       "      <th>2413</th>\n",
       "      <th>2663</th>\n",
       "      <th>2726</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    16    64    236   262   347   373   489   620   627   691   ...  2065  \\\n",
       "55    -1     0    -1     1     0    -1     0     0     1    -1  ...     1   \n",
       "88     0     0     0     1    -1     0     0     0     0    -1  ...    -1   \n",
       "26     1     0     1     1     0     0     0     1     0     0  ...    -1   \n",
       "42     1    -1     1     1    -1     1     1     0     0     0  ...    -1   \n",
       "69     1     0     1     1     0     0     0     0     0     0  ...    -1   \n",
       "..   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   ...   \n",
       "60     0    -1     0     0     0     0     0     0     0     0  ...    -1   \n",
       "71     1     0     1     1     0     0     0     0     0     0  ...    -1   \n",
       "14     0     1     1     1     0     0     1     1     1     0  ...     0   \n",
       "92     1     0     1     1     0     1     1     0     1     0  ...     0   \n",
       "51     1     0     1     1     0     0     0     0     0     0  ...     0   \n",
       "\n",
       "    2155  2172  2184  2212  2213  2319  2413  2663  2726  \n",
       "55    -1    -1     1     0     0     1    -1     0     0  \n",
       "88     0     0     0     0     0     0     0     0     0  \n",
       "26    -1     2     2    -1    -1     0    -1     0     0  \n",
       "42    -1    -1    -1    -1    -1     1    -1    -1    -1  \n",
       "69     2     2     0    -1    -1     1     0    -1     0  \n",
       "..   ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "60     0     1     2     0     0     0     0     0     0  \n",
       "71     1     0     2     1    -1     1    -1     0     0  \n",
       "14     0     0     0     0     0     0     0     1     1  \n",
       "92     0     0     0     0     0     0     1     0     1  \n",
       "51     0     0     0     0     0     0    -1     0     0  \n",
       "\n",
       "[80 rows x 33 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get index of selected features\n",
    "selected_features_list= list(fs_mutinfo.T.columns)\n",
    "\n",
    "# fit X_train and X_test with selected features\n",
    "X_train_fs_mutinfo = X_train.filter(items=selected_features_list, axis=1)\n",
    "X_test_fs_mutinfo = X_test.filter(items=selected_features_list, axis=1)\n",
    "X_train_fs_mutinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check overall distribution based on mutual information\n",
    "# fs_mutinfo = SelectKBest(score_func=mutual_info_classif,k='all')\n",
    "# fs_mutinfo.fit(X_train, y_train)\n",
    "\n",
    "# # X_train_fs = fs_mutinfo.transform(X_train)\n",
    "# # X_test_fs = fs.transform(X_test)\n",
    "\n",
    "# # check overall distribution based on mutual information 1\n",
    "# fs_mutinfo1 = SelectKBest(score_func=mutual_info_classif,k='all')\n",
    "# fs_mutinfo1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # what are scores for the features\n",
    "# for i in range(len(fs_mutinfo.scores_)):\n",
    "# \tprint('Feature %d: %f' % (i, fs_mutinfo.scores_[i]))\n",
    "# # plot the scores\n",
    "# figure(figsize=(10,8))\n",
    "# pyplot.bar([i for i in range(len(fs_mutinfo.scores_))], fs_mutinfo.scores_)\n",
    "# pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MI >= 0.05\n",
    "# # calculate the mean score\n",
    "# fs_mutinfo_all_df = pd.DataFrame(fs_mutinfo.scores_)\n",
    "# fs_mutinfo_all_df.rename(columns={0:'Scores'}, inplace=True)\n",
    "# mean_mutinfo = fs_mutinfo_all_df['Scores'].mean()\n",
    "# print('Mean score of mutual information:', mean_mutinfo)\n",
    "\n",
    "# # select features higher or equal to the mean\n",
    "# fs_mutinfo = fs_mutinfo_all_df.loc[fs_mutinfo_all_df['Scores'] >= 0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MI >= 0.07\n",
    "# # calculate the mean score\n",
    "# fs_mutinfo_all_df1 = pd.DataFrame(fs_mutinfo1.scores_)\n",
    "# fs_mutinfo_all_df1.rename(columns={0:'Scores'}, inplace=True)\n",
    "# mean_mutinfo1 = fs_mutinfo_all_df1['Scores'].mean()\n",
    "# print('Mean score of mutual information:', mean_mutinfo1)\n",
    "\n",
    "# # select features higher or equal to the mean\n",
    "# fs_mutinfo1 = fs_mutinfo_all_df1.loc[fs_mutinfo_all_df1['Scores'] >= 0.07]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # all features that have mutual information scores equal or above 0.05\n",
    "# fs_mutinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # all features that have mutual information scores equal or above 0.07\n",
    "# fs_mutinfo1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Select the top features (features with mutual information score >= 0.05)\n",
    "# fs_mutinfo = SelectKBest(score_func=mutual_info_classif, k=889)\n",
    "# fs_mutinfo.fit(X_train, y_train)\n",
    "\n",
    "# # list all selected features by from mutual information\n",
    "# features_mutinfo = fs_mutinfo.get_feature_names_out()\n",
    "\n",
    "# #####\n",
    "# # Select the top features (features with mutual information score >= 0.07)\n",
    "# fs_mutinfo1 = SelectKBest(score_func=mutual_info_classif, k=639)\n",
    "# fs_mutinfo1.fit(X_train, y_train)\n",
    "\n",
    "# # list all selected features by from mutual information\n",
    "# features_mutinfo1 = fs_mutinfo1.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get selected features for train and test sets\n",
    "# X_train_fs_mutinfo = pd.DataFrame(fs_mutinfo.transform(X_train))\n",
    "# X_test_fs_mutinfo = pd.DataFrame(fs_mutinfo.transform(X_test))\n",
    "\n",
    "# # rename column names for train set \n",
    "# for i, col in enumerate(X_train_fs_mutinfo.columns):\n",
    "#     X_train_fs_mutinfo.rename(columns={col:features_mutinfo[i]}, inplace=True)\n",
    "    \n",
    "# # rename column names for test set \n",
    "# for i, col in enumerate(X_test_fs_mutinfo.columns):\n",
    "#     X_test_fs_mutinfo.rename(columns={col:features_mutinfo[i]}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get selected features for train and test sets 1\n",
    "# X_train_fs_mutinfo1 = pd.DataFrame(fs_mutinfo1.transform(X_train))\n",
    "# X_test_fs_mutinfo1 = pd.DataFrame(fs_mutinfo1.transform(X_test))\n",
    "\n",
    "# # rename column names for train set \n",
    "# for i, col in enumerate(X_train_fs_mutinfo1.columns):\n",
    "#     X_train_fs_mutinfo1.rename(columns={col:features_mutinfo1[i]}, inplace=True)\n",
    "    \n",
    "# # rename column names for test set \n",
    "# for i, col in enumerate(X_test_fs_mutinfo1.columns):\n",
    "#     X_test_fs_mutinfo1.rename(columns={col:features_mutinfo1[i]}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nested Cross-validation\n",
    "- perform inner loop again on the whole training data to find the optimal hyperparameters\n",
    "- perform nested cross-validation to validate the model building strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean score using nested cross-validation is: 0.875 +/- 0.064\n",
      "The best parameters found are: {'gamma': 0.3, 'max_depth': 3, 'min_child_weight': 4, 'reg_alpha': 1e-05}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "# parameters for xgb\n",
    "param_grid_xgb = {\n",
    "    'max_depth': range(3,10),\n",
    "    'min_child_weight': range(1,6),\n",
    "    'gamma': [i/10.0 for i in range(0,5)],\n",
    "    # 'subsample': [i/10.0 for i in range(6,10)],\n",
    "    # 'colsample_bytree': [i/10.0 for i in range(6,10)],\n",
    "    'reg_alpha': [1e-5, 1e-2, 0.1, 1, 100]\n",
    "}\n",
    "\n",
    "# define xgb model\n",
    "model_to_tune_xgb = XGBClassifier(random_state=42,num_class=3,objective='multi:softmax')\n",
    "\n",
    "# Declare the inner and outer cross-validation strategies\n",
    "inner_cv_xgb = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "outer_cv_xgb = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Inner cross-validation for parameter search\n",
    "model_xgb = GridSearchCV(\n",
    "    estimator=model_to_tune_xgb, param_grid=param_grid_xgb, cv=inner_cv_xgb, n_jobs=2\n",
    ")\n",
    "\n",
    "model_xgb.fit(X_train_fs_mutinfo, lc_y_train)\n",
    "\n",
    "# Outer cross-validation to compute the testing score\n",
    "test_score_xgb = cross_val_score(model_xgb, X_train_fs_mutinfo, lc_y_train, cv=outer_cv_xgb, n_jobs=2)\n",
    "\n",
    "print(f\"The mean score using nested cross-validation is: \"\n",
    "      f\"{test_score_xgb.mean():.3f} +/- {test_score_xgb.std():.3f}\")\n",
    "print(f\"The best parameters found are: {model_xgb.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean score using nested cross-validation is: 0.800 +/- 0.037\n",
      "The best parameters found are: {'bootstrap': False, 'max_depth': 4, 'min_samples_leaf': 5, 'n_estimators': 5}\n"
     ]
    }
   ],
   "source": [
    "# parameters for random forest\n",
    "param_grid_rf = {\n",
    "    'max_depth': range(3,10),\n",
    "    'n_estimators': [1,5,10,15,20],\n",
    "    'min_samples_leaf': [1,2,3,4,5],\n",
    "    'bootstrap': [True, False]\n",
    "    }\n",
    "\n",
    "# define xgb model\n",
    "model_to_tune_rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Declare the inner and outer cross-validation strategies\n",
    "inner_cv_rf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "outer_cv_rf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Inner cross-validation for parameter search\n",
    "model_rf = GridSearchCV(\n",
    "    estimator = model_to_tune_rf, param_grid=param_grid_rf, cv=inner_cv_rf, n_jobs=2\n",
    ")\n",
    "\n",
    "model_rf.fit(X_train_fs_mutinfo, y_train)\n",
    "\n",
    "# Outer cross-validation to compute the testing score\n",
    "test_score_rf = cross_val_score(model_rf, X_train_fs_mutinfo, y_train, cv=outer_cv_rf, n_jobs=2)\n",
    "\n",
    "print(f\"The mean score using nested cross-validation is: \"\n",
    "      f\"{test_score_rf.mean():.3f} +/- {test_score_rf.std():.3f}\")\n",
    "print(f\"The best parameters found are: {model_rf.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaronlin/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/aaronlin/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/aaronlin/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/aaronlin/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/aaronlin/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/aaronlin/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "100 fits failed out of a total of 200.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aaronlin/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/aaronlin/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1091, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/aaronlin/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 61, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/aaronlin/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:953: UserWarning: One or more of the test scores are non-finite: [   nan 0.375     nan 0.375     nan 0.425     nan 0.525     nan 0.725\n",
      "    nan 0.7875    nan 0.8375    nan 0.8375    nan 0.8375    nan 0.85\n",
      "    nan 0.85      nan 0.8       nan 0.7875    nan 0.7625    nan 0.75\n",
      "    nan 0.7375    nan 0.7375    nan 0.7375    nan 0.7375    nan 0.7375]\n",
      "  warnings.warn(\n",
      "/Users/aaronlin/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "100 fits failed out of a total of 200.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aaronlin/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/aaronlin/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1091, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/aaronlin/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 61, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/aaronlin/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:953: UserWarning: One or more of the test scores are non-finite: [       nan 0.20545455        nan 0.20545455        nan 0.20545455\n",
      "        nan 0.29636364        nan 0.47090909        nan 0.73454545\n",
      "        nan 0.86909091        nan 0.88727273        nan 0.88909091\n",
      "        nan 0.87090909        nan 0.83272727        nan 0.83272727\n",
      "        nan 0.85272727        nan 0.85272727        nan 0.85090909\n",
      "        nan 0.83090909        nan 0.83090909        nan 0.83090909\n",
      "        nan 0.81090909        nan 0.81272727]\n",
      "  warnings.warn(\n",
      "/Users/aaronlin/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "100 fits failed out of a total of 200.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aaronlin/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/aaronlin/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1091, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/aaronlin/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 61, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/aaronlin/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:953: UserWarning: One or more of the test scores are non-finite: [       nan 0.3               nan 0.3               nan 0.3\n",
      "        nan 0.37454545        nan 0.47090909        nan 0.66181818\n",
      "        nan 0.7               nan 0.7               nan 0.68\n",
      "        nan 0.73454545        nan 0.67818182        nan 0.69818182\n",
      "        nan 0.68              nan 0.66              nan 0.66\n",
      "        nan 0.64              nan 0.64              nan 0.64\n",
      "        nan 0.64              nan 0.64      ]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean score using nested cross-validation is: 0.775 +/- 0.091\n",
      "The best parameters found are: {'C': 0.615848211066026, 'penalty': 'l2', 'solver': 'lbfgs'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaronlin/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "100 fits failed out of a total of 200.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aaronlin/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/aaronlin/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1091, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/aaronlin/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 61, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/aaronlin/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:953: UserWarning: One or more of the test scores are non-finite: [       nan 0.42545455        nan 0.42545455        nan 0.42545455\n",
      "        nan 0.42545455        nan 0.55454545        nan 0.72545455\n",
      "        nan 0.87090909        nan 0.88909091        nan 0.86909091\n",
      "        nan 0.85090909        nan 0.81454545        nan 0.77636364\n",
      "        nan 0.75818182        nan 0.74              nan 0.72181818\n",
      "        nan 0.70363636        nan 0.70363636        nan 0.70363636\n",
      "        nan 0.70363636        nan 0.70363636]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# parameters for logistic regression\n",
    "\n",
    "param_grid_lr = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': np.logspace(-4, 4, 20),\n",
    "    'solver': ['lbfgs']\n",
    "}\n",
    "\n",
    "# define xgb model\n",
    "model_to_tune_lr = LogisticRegression(multi_class='multinomial')\n",
    "\n",
    "# Declare the inner and outer cross-validation strategies\n",
    "inner_cv_lr = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "outer_cv_lr = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Inner cross-validation for parameter search\n",
    "model_lr = GridSearchCV(\n",
    "    estimator=model_to_tune_lr, param_grid=param_grid_lr, cv=inner_cv_lr, n_jobs=2\n",
    ")\n",
    "\n",
    "model_lr.fit(X_train_fs_mutinfo, y_train)\n",
    "\n",
    "# Outer cross-validation to compute the testing score\n",
    "test_score_lr = cross_val_score(model_lr, X_train_fs_mutinfo, y_train, cv=outer_cv_lr, n_jobs=2)\n",
    "\n",
    "print(f\"The mean score using nested cross-validation is: \"\n",
    "      f\"{test_score_lr.mean():.3f} +/- {test_score_lr.std():.3f}\")\n",
    "print(f\"The best parameters found are: {model_lr.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "all_scores = {\n",
    "    \"XGBoost\": test_score_xgb,\n",
    "    \"Random Forest\": test_score_rf,\n",
    "    \"Logistic Regression\": test_score_lr\n",
    "}\n",
    "all_scores = pd.DataFrame(all_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "color = {\"whiskers\": \"black\", \"medians\": \"black\", \"caps\": \"black\"}\n",
    "all_scores.plot.box(color=color, vert=False)\n",
    "plt.xlabel(\"Accuracy\")\n",
    "_ = plt.title(\"Comparison of mean accuracy on the nested cross-validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearchCV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from scipy.stats import loguniform\n",
    "from sklearn import metrics   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.9\n",
      "Best Hyperparameters: {'gamma': 0.2, 'max_depth': 6, 'min_child_weight': 1, 'reg_alpha': 1e-05}\n",
      "{'base_score': 0.5,\n",
      " 'booster': 'gbtree',\n",
      " 'callbacks': None,\n",
      " 'colsample_bylevel': 1,\n",
      " 'colsample_bynode': 1,\n",
      " 'colsample_bytree': 1,\n",
      " 'early_stopping_rounds': None,\n",
      " 'enable_categorical': False,\n",
      " 'eval_metric': None,\n",
      " 'gamma': 0.2,\n",
      " 'gpu_id': -1,\n",
      " 'grow_policy': 'depthwise',\n",
      " 'importance_type': None,\n",
      " 'interaction_constraints': '',\n",
      " 'learning_rate': 0.300000012,\n",
      " 'max_bin': 256,\n",
      " 'max_cat_to_onehot': 4,\n",
      " 'max_delta_step': 0,\n",
      " 'max_depth': 6,\n",
      " 'max_leaves': 0,\n",
      " 'min_child_weight': 1,\n",
      " 'missing': nan,\n",
      " 'monotone_constraints': '()',\n",
      " 'n_estimators': 100,\n",
      " 'n_jobs': 0,\n",
      " 'num_class': 3,\n",
      " 'num_parallel_tree': 1,\n",
      " 'objective': 'multi:softmax',\n",
      " 'predictor': 'auto',\n",
      " 'random_state': 42,\n",
      " 'reg_alpha': 1e-05,\n",
      " 'reg_lambda': 1,\n",
      " 'sampling_method': 'uniform',\n",
      " 'scale_pos_weight': None,\n",
      " 'subsample': 1,\n",
      " 'tree_method': 'exact',\n",
      " 'use_label_encoder': False,\n",
      " 'validate_parameters': 1,\n",
      " 'verbosity': None}\n"
     ]
    }
   ],
   "source": [
    "# define model 1\n",
    "model_xgb = XGBClassifier(random_state=42,num_class=3,objective='multi:softmax')\n",
    "\n",
    "# define evaluation\n",
    "# cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define search space\n",
    "space_xgb = dict()\n",
    "space_xgb['max_depth'] = range(3,10)\n",
    "space_xgb['min_child_weight'] = range(1,6)\n",
    "space_xgb['gamma'] = [i/10.0 for i in range(0,5)]\n",
    "# space_xgb['subsample'] = [i/10.0 for i in range(6,10)]\n",
    "# space_xgb['colsample_bytree'] = [i/10.0 for i in range(6,10)]\n",
    "space_xgb['reg_alpha'] = [1e-5, 1e-2, 0.1, 1, 100]\n",
    "\n",
    "# define search_xgb\n",
    "search_xgb = GridSearchCV(model_xgb, space_xgb, scoring='accuracy', n_jobs=2, cv=5)\n",
    "\n",
    "# execute search_xgb\n",
    "result_xgb = search_xgb.fit(X_train_fs_mutinfo, lc_y_train)\n",
    "\n",
    "# summarize result_xgb\n",
    "print('Best Score: %s' % result_xgb.best_score_)\n",
    "print('Best Hyperparameters: %s' % result_xgb.best_params_)\n",
    "\n",
    "# print winning set of hyperparameters\n",
    "from pprint import pprint\n",
    "pprint(result_xgb.best_estimator_.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 350 candidates, totalling 1750 fits\n",
      "Best Score: 0.8625\n",
      "Best Hyperparameters: {'bootstrap': True, 'max_depth': 4, 'min_samples_leaf': 1, 'n_estimators': 15}\n",
      "{'bootstrap': True,\n",
      " 'ccp_alpha': 0.0,\n",
      " 'class_weight': None,\n",
      " 'criterion': 'gini',\n",
      " 'max_depth': 4,\n",
      " 'max_features': 'sqrt',\n",
      " 'max_leaf_nodes': None,\n",
      " 'max_samples': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'n_estimators': 15,\n",
      " 'n_jobs': None,\n",
      " 'oob_score': False,\n",
      " 'random_state': 42,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "# define model 1\n",
    "model_rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# define evaluation\n",
    "# cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define search space\n",
    "space_rf = dict()\n",
    "space_rf['max_depth'] = range(3,10)\n",
    "space_rf['n_estimators'] = [1,5,10,15,20]\n",
    "space_rf['min_samples_leaf'] = [1,2,3,4,5]\n",
    "space_rf['bootstrap'] = [True, False]\n",
    "\n",
    "# define search_rfspace_rf\n",
    "search_space_rf = GridSearchCV(model_rf, space_rf, scoring='accuracy', n_jobs=-1, cv=5, verbose=True)\n",
    "\n",
    "# execute search_rfspace_rf\n",
    "result_rf = search_space_rf.fit(X_train_fs_mutinfo, y_train)\n",
    "\n",
    "# summarize result_rfspace_rf\n",
    "print('Best Score: %s' % result_rf.best_score_)\n",
    "print('Best Hyperparameters: %s' % result_rf.best_params_)\n",
    "\n",
    "# print winning set of hyperparameters\n",
    "from pprint import pprint\n",
    "pprint(result_rf.best_estimator_.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "Best Score: 0.8375\n",
      "Best Hyperparameters: {'C': 0.03359818286283781, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "{'C': 0.03359818286283781,\n",
      " 'class_weight': None,\n",
      " 'dual': False,\n",
      " 'fit_intercept': True,\n",
      " 'intercept_scaling': 1,\n",
      " 'l1_ratio': None,\n",
      " 'max_iter': 100,\n",
      " 'multi_class': 'multinomial',\n",
      " 'n_jobs': None,\n",
      " 'penalty': 'l2',\n",
      " 'random_state': None,\n",
      " 'solver': 'lbfgs',\n",
      " 'tol': 0.0001,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaronlin/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/aaronlin/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/aaronlin/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/aaronlin/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "100 fits failed out of a total of 200.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aaronlin/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/aaronlin/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1091, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/aaronlin/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 61, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/aaronlin/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:953: UserWarning: One or more of the test scores are non-finite: [   nan 0.3625    nan 0.3625    nan 0.3625    nan 0.5875    nan 0.7625\n",
      "    nan 0.825     nan 0.8375    nan 0.8       nan 0.75      nan 0.7625\n",
      "    nan 0.7625    nan 0.75      nan 0.7625    nan 0.7375    nan 0.675\n",
      "    nan 0.6625    nan 0.65      nan 0.65      nan 0.65      nan 0.65  ]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# define model 1\n",
    "model_lr = LogisticRegression(multi_class='multinomial')\n",
    "\n",
    "# define evaluation\n",
    "# cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define search space\n",
    "space_lr = dict()\n",
    "space_lr['penalty'] = ['l1', 'l2']\n",
    "space_lr['C'] = np.logspace(-4, 4, 20)\n",
    "space_lr['solver'] = ['lbfgs']\n",
    "\n",
    "\n",
    "# define search_lrspace_lrspace_lr\n",
    "search_space_lr = GridSearchCV(model_lr, space_lr, scoring='accuracy', n_jobs=-1, cv=5, verbose=True)\n",
    "\n",
    "# execute search_lrspace_lrspace_lr\n",
    "result_lr = search_space_lr.fit(X_train_fs_mutinfo, y_train)\n",
    "\n",
    "# summarize result_lrspace_lrspace_lr\n",
    "print('Best Score: %s' % result_lr.best_score_)\n",
    "print('Best Hyperparameters: %s' % result_lr.best_params_)\n",
    "\n",
    "# print winning set of hyperparameters\n",
    "from pprint import pprint\n",
    "pprint(result_lr.best_estimator_.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "### final optimal models train on all train set\n",
    "\n",
    "# XGBoost\n",
    "opt_xgb = result_xgb.best_estimator_\n",
    "final_opt_xgb = opt_xgb.fit(X_train_fs_mutinfo, lc_y_train)\n",
    "\n",
    "# Random Forest\n",
    "opt_rf = result_rf.best_estimator_\n",
    "final_opt_rf = opt_rf.fit(X_train_fs_mutinfo, y_train)\n",
    "\n",
    "# Logistic Regression\n",
    "opt_lr = result_lr.best_estimator_\n",
    "final_opt_lr = opt_lr.fit(X_train_fs_mutinfo, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([  16,   64,  236,  262,  347,  373,  489,  620,  627,  691,  694,\n",
       "             830, 1001, 1123, 1474, 1563, 1592, 1646, 1656, 1673, 1689, 1726,\n",
       "            1912, 2065, 2155, 2172, 2184, 2212, 2213, 2319, 2413, 2663, 2726],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_fs_mutinfo.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Aligned feature names \n",
    "\n",
    "X_train_fs_mutinfo_list = list(X_train_fs_mutinfo.columns)\n",
    "\n",
    "# Random Forest\n",
    "feature_importances_rf = pd.Series(opt_rf.feature_importances_)\n",
    "feature_importances_rf.index = X_train_fs_mutinfo_list\n",
    "\n",
    "# Logist Regression\n",
    "feature_importances_lr = pd.Series(opt_lr.coef_[0])\n",
    "feature_importances_lr.index = X_train_fs_mutinfo_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importances\n",
    "\n",
    "# XGBoost\n",
    "feature_importances_xgb = pd.Series(opt_xgb.get_booster().get_score(importance_type='gain'))\n",
    "feature_importances_xgb.sort_values(ascending=False).to_csv('XGB_feature_importance.csv')\n",
    "\n",
    "# Random Forest\n",
    "feature_importances_rf.sort_values(ascending=False).to_csv('RF_feature_importance.csv')\n",
    "\n",
    "# Logistic Regression\n",
    "feature_importances_lr.sort_values(ascending=False).to_csv('LR_feature_importance.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-label confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost prediction\n",
    "y_pred_train_xgb = final_opt_xgb.predict(X_train_fs_mutinfo)\n",
    "\n",
    "# Random Forest prediction\n",
    "y_pred_train_rf = final_opt_rf.predict(X_train_fs_mutinfo)\n",
    "\n",
    "# Logistic Regression prediction\n",
    "y_pred_train_lr = final_opt_lr.predict(X_train_fs_mutinfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for XGBoost\n",
    "confusion_mx_train_xgb = confusion_matrix(lc_y_train, y_pred_train_xgb)\n",
    "\n",
    "# Confusion matrix for Random Forest\n",
    "confusion_mx_train_rf = confusion_matrix(y_train, y_pred_train_rf)\n",
    "\n",
    "# Confusion matrix for Logistic Regression\n",
    "confusion_mx_train_lr = confusion_matrix(y_train, y_pred_train_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframe for a array-formatted Confusion matrix,so it will be easy for plotting.\n",
    "\n",
    "# XGBoost\n",
    "confusion_mx_train_xgb_df = pd.DataFrame(confusion_mx_train_xgb,\n",
    "                     index = ['HER2+','HR+','Triple Neg'], \n",
    "                     columns = ['HER2+','HR+','Triple Neg'])\n",
    "\n",
    "# Random Forest\n",
    "confusion_mx_train_rf_df = pd.DataFrame(confusion_mx_train_rf,\n",
    "                     index = ['HER2+','HR+','Triple Neg'], \n",
    "                     columns = ['HER2+','HR+','Triple Neg'])\n",
    "\n",
    "# Logistic Regression\n",
    "confusion_mx_train_lr_df = pd.DataFrame(confusion_mx_train_lr,\n",
    "                     index = ['HER2+','HR+','Triple Neg'], \n",
    "                     columns = ['HER2+','HR+','Triple Neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting the confusion matrix - XGBoost\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(confusion_mx_train_xgb_df, annot=True)\n",
    "plt.title('XGBoost (training set)')\n",
    "plt.ylabel('Actal Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting the confusion matrix - Random Forest\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(confusion_mx_train_rf_df, annot=True)\n",
    "plt.title('Random Forest (training set)')\n",
    "plt.ylabel('Actal Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting the confusion matrix - Logistic Regression\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(confusion_mx_train_lr_df, annot=True)\n",
    "plt.title('Logistic Regression (training set)')\n",
    "plt.ylabel('Actal Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-label F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score - XGBoost:  0.9878143347734575\n",
      "f1 score - Random Forest:  0.9507168458781362\n",
      "f1 score - Logistic Regression:  0.9390716738672871\n",
      "precision - XGBoost:  0.9888888888888889\n",
      "precision- Random Forest:  0.9595959595959597\n",
      "precision- Logistic Regression:  0.94\n",
      "recall - XGBoost:  0.9871794871794872\n",
      "recall - Random Forest:  0.9487179487179488\n",
      "recall - Logistic Regression:  0.938549955791335\n",
      "accuracy - XGBoost:  0.9875\n",
      "accuracy - Random Forest:  0.95\n",
      "accuracy - Logistic Regression:  0.9375\n"
     ]
    }
   ],
   "source": [
    "# get f1 score (macro average)\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score_xgb = f1_score(lc_y_train, y_pred_train_xgb, average='macro')\n",
    "f1_score_rf = f1_score(y_train, y_pred_train_rf, average='macro')\n",
    "f1_score_lr = f1_score(y_train, y_pred_train_lr, average='macro')\n",
    "\n",
    "precision_xgb = precision_score(lc_y_train, y_pred_train_xgb, average='macro')\n",
    "precision_rf = precision_score(y_train, y_pred_train_rf, average='macro')\n",
    "precision_lr = precision_score(y_train, y_pred_train_lr, average='macro')\n",
    "\n",
    "recall_xgb = recall_score(lc_y_train, y_pred_train_xgb, average='macro')\n",
    "recall_rf = recall_score(y_train, y_pred_train_rf, average='macro')\n",
    "recall_lr = recall_score(y_train, y_pred_train_lr, average='macro')\n",
    "\n",
    "accuracy_xgb = accuracy_score(lc_y_train, y_pred_train_xgb)\n",
    "accuracy_rf = accuracy_score(y_train, y_pred_train_rf)\n",
    "accuracy_lr = accuracy_score(y_train, y_pred_train_lr)\n",
    "\n",
    "print('f1 score - XGBoost: ', f1_score_xgb)\n",
    "print('f1 score - Random Forest: ', f1_score_rf)\n",
    "print('f1 score - Logistic Regression: ', f1_score_lr)\n",
    "\n",
    "print('precision - XGBoost: ', precision_xgb)\n",
    "print('precision- Random Forest: ', precision_rf)\n",
    "print('precision- Logistic Regression: ', precision_lr)\n",
    "\n",
    "print('recall - XGBoost: ', recall_xgb)\n",
    "print('recall - Random Forest: ', recall_rf)\n",
    "print('recall - Logistic Regression: ', recall_lr)\n",
    "\n",
    "print('accuracy - XGBoost: ', accuracy_xgb)\n",
    "print('accuracy - Random Forest: ', accuracy_rf)\n",
    "print('accuracy - Logistic Regression: ', accuracy_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation (test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on each model\n",
    "\n",
    "# XGBoost\n",
    "y_pred_test_xgb = final_opt_xgb.predict(X_test_fs_mutinfo)\n",
    "\n",
    "# Random Forest\n",
    "y_pred_test_rf = final_opt_rf.predict(X_test_fs_mutinfo)\n",
    "\n",
    "# Logistic Regression\n",
    "y_pred_test_lr = final_opt_lr.predict(X_test_fs_mutinfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make confustion matrix for each model on test set\n",
    "\n",
    "# XGBoost\n",
    "confusion_mx_test_xgb = confusion_matrix(lc_y_test, y_pred_test_xgb)\n",
    "\n",
    "# Random Forest\n",
    "confusion_mx_test_rf = confusion_matrix(y_test, y_pred_test_rf)\n",
    "\n",
    "# Logistic Regression\n",
    "confusion_mx_test_lr = confusion_matrix(y_test, y_pred_test_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframe for a array-formatted Confusion matrix,so it will be easy for plotting.\n",
    "\n",
    "# XGBoost\n",
    "confusion_mx_test_xgb_df = pd.DataFrame(confusion_mx_test_xgb,\n",
    "                     index = ['HER2+','HR+','Triple Neg'], \n",
    "                     columns = ['HER2+','HR+','Triple Neg'])\n",
    "\n",
    "# Random Forest\n",
    "confusion_mx_test_rf_df = pd.DataFrame(confusion_mx_test_rf,\n",
    "                     index = ['HER2+','HR+','Triple Neg'], \n",
    "                     columns = ['HER2+','HR+','Triple Neg'])\n",
    "\n",
    "# Logistic Regression\n",
    "confusion_mx_test_lr_df = pd.DataFrame(confusion_mx_test_lr,\n",
    "                     index = ['HER2+','HR+','Triple Neg'], \n",
    "                     columns = ['HER2+','HR+','Triple Neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting the confusion matrix - XGBoost\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(confusion_mx_test_xgb_df, annot=True)\n",
    "plt.title('XGBoost (set)')\n",
    "plt.ylabel('Actal values')\n",
    "plt.xlabel('Predicted values')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting the confusion matrix - Random Forest\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(confusion_mx_test_rf_df, annot=True)\n",
    "plt.title('Random Forest (test set)')\n",
    "plt.ylabel('Actal Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting the confusion matrix - Logistic Regression\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(confusion_mx_test_lr_df, annot=True)\n",
    "plt.title('Logistic Regression (test set)')\n",
    "plt.ylabel('Actal Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HR+           7\n",
       "HER2+         7\n",
       "Triple Neg    6\n",
       "Name: Subgroup, dtype: int64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score:  0.8300653594771242\n",
      "Precision:  0.9\n",
      "Recall:  0.8333333333333334\n",
      "Accuracy:  0.85\n"
     ]
    }
   ],
   "source": [
    "# Final result - XGBoost\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "f1_xgb_test = f1_score(lc_y_test, y_pred_test_xgb, average='macro')\n",
    "precision_xgb_test = precision_score(lc_y_test, y_pred_test_xgb, average='macro')\n",
    "recall_xgb_test = recall_score(lc_y_test, y_pred_test_xgb, average='macro')\n",
    "accuracy_xgb_test = accuracy_score(lc_y_test, y_pred_test_xgb)\n",
    "\n",
    "print('f1-score: ', f1_xgb_test)\n",
    "print('Precision: ', precision_xgb_test)\n",
    "print('Recall: ', recall_xgb_test)\n",
    "print('Accuracy: ', accuracy_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score:  0.8424242424242423\n",
      "Precision:  0.85\n",
      "Recall:  0.8412698412698413\n",
      "Accuracy:  0.85\n"
     ]
    }
   ],
   "source": [
    "# Final result - Random Forest\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "f1_rf_test = f1_score(y_test, y_pred_test_rf, average='macro')\n",
    "precision_rf_test = precision_score(y_test, y_pred_test_rf, average='macro')\n",
    "recall_rf_test = recall_score(y_test, y_pred_test_rf, average='macro')\n",
    "accuracy_rf_test = accuracy_score(y_test, y_pred_test_rf)\n",
    "\n",
    "print('f1-score: ', f1_rf_test)\n",
    "print('Precision: ', precision_rf_test)\n",
    "print('Recall: ', recall_rf_test)\n",
    "print('Accuracy: ', accuracy_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score:  0.8300653594771242\n",
      "Precision:  0.9\n",
      "Recall:  0.8333333333333334\n",
      "Accuracy:  0.85\n"
     ]
    }
   ],
   "source": [
    "# Final result - Logistic Regression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "f1_lr_test = f1_score(y_test, y_pred_test_lr, average='macro')\n",
    "precision_lr_test = precision_score(y_test, y_pred_test_lr, average='macro')\n",
    "recall_lr_test = recall_score(y_test, y_pred_test_lr, average='macro')\n",
    "accuracy_lr_test = accuracy_score(y_test, y_pred_test_lr)\n",
    "\n",
    "print('f1-score: ', f1_lr_test)\n",
    "print('Precision: ', precision_lr_test)\n",
    "print('Recall: ', recall_lr_test)\n",
    "print('Accuracy: ', accuracy_lr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import ClassPredictionError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Feature shape mismatch, expected: 33, got 2834",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/d1/335rkct13zx_60rbbgcmn8yc0000gn/T/ipykernel_35251/2972516895.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Evaluate the model on the test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mvisualizer_xgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Draw visualization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/yellowbrick/classifier/class_prediction_error.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;31m# Must be computed before calling super\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;31m# We're relying on predict to raise NotFitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, output_margin, ntree_limit, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0miteration_range\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m     ) -> np.ndarray:\n\u001b[0;32m-> 1434\u001b[0;31m         class_probs = super().predict(\n\u001b[0m\u001b[1;32m   1435\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1436\u001b[0m             \u001b[0moutput_margin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_margin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, output_margin, ntree_limit, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[1;32m   1047\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_use_inplace_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m                 predts = self.get_booster().inplace_predict(\n\u001b[0m\u001b[1;32m   1050\u001b[0m                     \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m                     \u001b[0miteration_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miteration_range\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36minplace_predict\u001b[0;34m(self, data, iteration_range, predict_type, missing, validate_features, base_margin, strict_shape)\u001b[0m\n\u001b[1;32m   2081\u001b[0m                 )\n\u001b[1;32m   2082\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2083\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m   2084\u001b[0m                     \u001b[0;34mf\"Feature shape mismatch, expected: {self.num_features()}, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2085\u001b[0m                     \u001b[0;34mf\"got {data.shape[1]}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Feature shape mismatch, expected: 33, got 2834"
     ]
    }
   ],
   "source": [
    "# Instantiate the classification model and visualizer\n",
    "classes_xgb = ['HER2+','HR+','Triple Neg']\n",
    "visualizer_xgb = ClassPredictionError(\n",
    "    opt_xgb, classes=classes_xgb\n",
    ")\n",
    "\n",
    "# Fit the training data to the visualizer\n",
    "visualizer_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "visualizer_xgb.score(X_test, y_test)\n",
    "\n",
    "# Draw visualization\n",
    "visualizer_xgb.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mark the biomarker (Chromosome 17(35076296 - 35282086) ) for classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Chr17 specific region as the only feature in the training and test sets\n",
    "chr17_train = X_train[2184]\n",
    "chr17_test = X_test[2184]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GridsearchCV\n",
    "# define mode\n",
    "model_xgb_chr17 = XGBClassifier(random_state=42,num_class=3,objective='multi:softmax')\n",
    "\n",
    "# define evaluation\n",
    "# cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define search space\n",
    "space_xgb = dict()\n",
    "space_xgb['max_depth'] = range(3,10)\n",
    "space_xgb['min_child_weight'] = range(1,6)\n",
    "space_xgb['gamma'] = [i/10.0 for i in range(0,5)]\n",
    "# space_xgb['subsample'] = [i/10.0 for i in range(6,10)]\n",
    "# space_xgb['colsample_bytree'] = [i/10.0 for i in range(6,10)]\n",
    "space_xgb['reg_alpha'] = [1e-5, 1e-2, 0.1, 1, 100]\n",
    "\n",
    "# define search_xgb\n",
    "search_xgb_chr17 = GridSearchCV(model_xgb, space_xgb, scoring='accuracy', n_jobs=-1, cv=5)\n",
    "\n",
    "# execute search_xgb\n",
    "result_xgb_chr17 = search_xgb_chr17.fit(chr17_train, lc_y_train)\n",
    "\n",
    "# summarize result_xgb\n",
    "print('Best Score: %s' % result_xgb_chr17.best_score_)\n",
    "print('Best Hyperparameters: %s' % result_xgb_chr17.best_params_)\n",
    "\n",
    "# print winning set of hyperparameters\n",
    "from pprint import pprint\n",
    "pprint(result_xgb_chr17.best_estimator_.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model evaluation on training set (chr17)\n",
    "# (maybe this step is unnecessary; because what is the point we train again and use the same training set to make predictions?)\n",
    "\n",
    "# XGBoost chr17\n",
    "opt_xgb_chr17 = result_xgb_chr17.best_estimator_\n",
    "final_opt_xgb_chr17 = opt_xgb_chr17.fit(chr17_train, lc_y_train)\n",
    "\n",
    "# XGBoost chr17 prediction (training set)\n",
    "y_pred_train_xgb_chr17 = final_opt_xgb_chr17.predict(chr17_train)\n",
    "\n",
    "# Confusion matrix for XGBoost chr17\n",
    "confusion_mx_train_xgb_chr17 = confusion_matrix(lc_y_train, y_pred_train_xgb_chr17)\n",
    "\n",
    "# Creating dataframe for a array-formatted Confusion matrix,so it will be easy for plotting.\n",
    "# XGBoost chr17\n",
    "confusion_mx_train_xgb_chr17_df = pd.DataFrame(confusion_mx_train_xgb_chr17,\n",
    "                     index = ['HER2+','HR+','Triple Neg'], \n",
    "                     columns = ['HER2+','HR+','Triple Neg'])\n",
    "\n",
    "\n",
    "#Plotting the confusion matrix - XGBoost chr17 (val set)\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(confusion_mx_train_xgb_chr17_df, annot=True)\n",
    "plt.title('XGBoost (feature chr17: 35076296 - 35282086) on training set)')\n",
    "plt.ylabel('Actal Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final result - with only ffeature chr17\n",
    "\n",
    "f1_chr17_train = f1_score(lc_y_train, y_pred_train_xgb_chr17, average='macro')\n",
    "precision_chr17_train = precision_score(lc_y_train, y_pred_train_xgb_chr17, average='macro')\n",
    "recall_chr17_train= recall_score(lc_y_train, y_pred_train_xgb_chr17, average='macro')\n",
    "accuracy_chr17_train = accuracy_score(lc_y_train, y_pred_train_xgb_chr17)\n",
    "\n",
    "print('f1-score: ', f1_chr17_train)\n",
    "print('Precision: ', precision_chr17_train)\n",
    "print('Recall: ', recall_chr17_train)\n",
    "print('Accuracy: ', accuracy_chr17_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model evaluation on test set (chr17)\n",
    "# (maybe this step is unnecessary; because what is the point we train again and use the same training set to make predictions?)\n",
    "\n",
    "# XGBoost chr17\n",
    "opt_xgb_chr17 = result_xgb_chr17.best_estimator_\n",
    "final_opt_xgb_chr17 = opt_xgb_chr17.fit(chr17_train, lc_y_train)\n",
    "\n",
    "# XGBoost chr17 prediction (test set)\n",
    "y_pred_test_xgb_chr17 = final_opt_xgb_chr17.predict(chr17_test)\n",
    "\n",
    "# Confusion matrix for XGBoost chr17\n",
    "confusion_mx_test_xgb_chr17 = confusion_matrix(lc_y_test, y_pred_test_xgb_chr17)\n",
    "\n",
    "# Creating dataframe for a array-formatted Confusion matrix,so it will be easy for plotting.\n",
    "# XGBoost chr17\n",
    "confusion_mx_test_xgb_chr17_df = pd.DataFrame(confusion_mx_test_xgb_chr17,\n",
    "                     index = ['HER2+','HR+','Triple Neg'], \n",
    "                     columns = ['HER2+','HR+','Triple Neg'])\n",
    "\n",
    "\n",
    "#Plotting the confusion matrix - XGBoost chr17 (test set)\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(confusion_mx_test_xgb_chr17_df, annot=True)\n",
    "plt.title('XGBoost (feature chr17: 35076296 - 35282086) on test set)')\n",
    "plt.ylabel('Actal testues')\n",
    "plt.xlabel('Predicted testues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final result - with only ffeature chr17\n",
    "\n",
    "f1_chr17_test = f1_score(lc_y_test, y_pred_test_xgb_chr17, average='macro')\n",
    "precision_chr17_test = precision_score(lc_y_test, y_pred_test_xgb_chr17, average='macro')\n",
    "recall_chr17_test = recall_score(lc_y_test, y_pred_test_xgb_chr17, average='macro')\n",
    "accuracy_chr17_test = accuracy_score(lc_y_test, y_pred_test_xgb_chr17)\n",
    "\n",
    "print('f1-score: ', f1_chr17_test)\n",
    "print('Precision: ', precision_chr17_test)\n",
    "print('Recall: ', recall_chr17_test)\n",
    "print('Accuracy: ', accuracy_chr17_test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "66576c1392964d49fa8b18b7b994b1bdb67ce0891e48b11232b852dfea8ef2a9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
